{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76d7efbc-d0ad-4755-8ef4-336f5177f640",
   "metadata": {},
   "source": [
    "# Atlantic-Eq Mode Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01527630-b4f9-466f-9611-8a3fb0ee47c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Librerías Estándar \n",
    "import random\n",
    "import warnings\n",
    "\n",
    "# Análisis de Datos y Matemáticas \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from scipy import stats\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "from netCDF4 import Dataset as ncread  # Asumiendo que usas ncread explícitamente\n",
    "\n",
    "# Visuals\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib import animation\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "#Utilidades y Entorno (Jupyter/IPython) ---\n",
    "from tabulate import tabulate\n",
    "from IPython.display import display, HTML, Image  # Todo en una sola línea\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2484670-0c68-4476-9ece-1154f9d9062a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'E:/TFG/Datos/Importants/'\n",
    "data1 = path+'HadISST_sst_v1.1_196001_202105.nc'#database_a: HadISST (Hadley Centre Sea Ice and Sea Surface Temperature dataset) de 1960-2021, 1°x1°\n",
    "data2 = path+'sst.mnmean_v5_196001_202105.nc'   #database_b: ERSST (Extended Reconstructed SST), 2°x2° from NOAA\n",
    "data3 = path+'noaa.pcp.mon.anom_196001_202105.nc' #datobase1: NOAA (National Oceanic and Atmospheric Administration ), 1.875°x2° \n",
    "data4 = path+'pcp.mon.ncep-ncar_196001_202105.nc' #datobase2: NCEP-NCAR, 2.5°x2.5°\n",
    "#Retrieve data\n",
    "nc1 = ncread(data1, 'r')  #sst\n",
    "nc2 = ncread(data2, 'r')  #sst\n",
    "nc3 = ncread(data3, 'r')  #prec\n",
    "nc4 = ncread(data4, 'r')  #prec\n",
    "#vamos a ver las variables que tenemos que llamar de cada archivo\n",
    "var1 = nc1.variables[\"sst\"]\n",
    "var2 = nc2.variables[\"sst\"]\n",
    "var3 = nc3.variables[\"precip\"]\n",
    "var4 = nc4.variables[\"prate\"]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def summarize_var(var, nt=None, p=(1, 99)):\n",
    "    x = var[:] if nt is None else var[:nt]\n",
    "    if isinstance(x, np.ma.MaskedArray):\n",
    "        masked = int(np.sum(x.mask))\n",
    "        x = np.ma.filled(x, np.nan)\n",
    "    else:\n",
    "        masked = 0\n",
    "    x = x.astype(float)\n",
    "    vals = x[np.isfinite(x)]\n",
    "    \n",
    "    return {\"units\": getattr(var, \"units\", None),\n",
    "        \"p1\": np.percentile(vals, p[0]) if vals.size else np.nan,\n",
    "        \"p99\": np.percentile(vals, p[1]) if vals.size else np.nan,\n",
    "        \"min\": np.min(vals) if vals.size else np.nan,\n",
    "        \"max\": np.max(vals) if vals.size else np.nan,\n",
    "        \"n_total\": x.size,\n",
    "        \"masked_count\": masked,\n",
    "        \"frac_finite\": vals.size / x.size if x.size else np.nan}\n",
    "    \n",
    "s1 = summarize_var(var1, nt=732)\n",
    "s2 = summarize_var(var2, nt=732)\n",
    "s3 = summarize_var(var3, nt=732)\n",
    "s4 = summarize_var(var4, nt=732)\n",
    "# Vamos a crear una tabla para mostrar la naturaleza de los datos:\n",
    "\n",
    "vars_dict = {\"HadISST SST\": var1,\"ERSST SST\": var2,\"NOAA PCP\": var3,\"NCEP-NCAR PCP\": var4}\n",
    "table = []\n",
    "for name, var in vars_dict.items():\n",
    "    s = summarize_var(var, nt=732)\n",
    "    table.append([name,s[\"units\"],(s[\"p1\"], s[\"p99\"]),(s[\"min\"], s[\"max\"]),s[\"frac_finite\"]])\n",
    "    columns = [\"Base de datos\",\"Unidades\",\"p1 / p99\",\"Min / Max\",\"Fracción válida\"]\n",
    "\n",
    "df_table = pd.DataFrame(table, columns=columns)\n",
    "df_table = df_table.set_index(\"Base de datos\")\n",
    "\n",
    "df_table[\"p1 / p99\"] = df_table[\"p1 / p99\"].apply(lambda x: f\"({x[0]:.2f}, {x[1]:.2f})\")\n",
    "\n",
    "df_table[\"Min / Max\"] = df_table[\"Min / Max\"].apply(lambda x: f\"({x[0]:.2f}, {x[1]:.2f})\")\n",
    "\n",
    "df_table[\"Fracción válida\"] = df_table[\"Fracción válida\"].apply(lambda x: f\"{x:.2f}\")\n",
    "print(tabulate(df_table, headers=\"keys\", tablefmt=\"pretty\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6273c7-94da-42cc-801e-98eac5867d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\"HadISST\": data1,\"ERSST\": data2,\"NOAA PCP\": data3,\"NCEP-NCAR\": data4}\n",
    "\n",
    "# Función para extraer info de los datos, resolución, rango, etc\n",
    "def ex_info(file_path):\n",
    "    \n",
    "    nc = ncread(file_path, \"r\")  # Abrir archivo NetCDF\n",
    "\n",
    "     # Verificamos cómo están nombradas las variables de latitud y longitud\n",
    "    lat_name = next(var for var in nc.variables.keys() if \"lat\" in var.lower())  #next devuele el primer elemento del iterador \n",
    "    lon_name = next(var for var in nc.variables.keys() if \"lon\" in var.lower())  #util porque no queremos una lista de 1 elemento queremos el string\n",
    "                                                                                     #lat_name[0] tambien devuelve el primero pero el next te evita\n",
    "    latitudes = nc.variables[lat_name][:]                                        #crear la lista, es mas eficiente\n",
    "    longitudes = nc.variables[lon_name][:]\n",
    "\n",
    "    # Rango de latitud y longitud\n",
    "    lat_range = (latitudes.min(), latitudes.max())\n",
    "    lon_range = (longitudes.min(), longitudes.max())\n",
    "\n",
    "    # Resolución (suponemos que la diferencia entre dos puntos consecutivos es la resolución)\n",
    "    lat_res = round(abs(latitudes[1] - latitudes[0]),3)\n",
    "    lon_res = round(abs(longitudes[1] - longitudes[0]),3)\n",
    "\n",
    "    nc.close()  # Cerrar el archivo para liberar memoria\n",
    "    return (lat_res, lon_res), lat_range, lon_range\n",
    "\n",
    "# Crear la tabla con los datos extraídos\n",
    "table = []\n",
    "for name, path in data_files.items(): #Este bucle recorre cada par (nombre, ruta) del diccionario.\n",
    "    info = ex_info(path)  #Pathh en este contexto seria el segundo numero de la dupla es decir data1,data2,etc\n",
    "    if info:\n",
    "        table.append([name, *info]) #con el * desempacamos la tupla y la tabla es [HadISST, (1,2),(3,4),(5,6)] y no [HadISST,[(1,2),(3,4),(5,6)]]\n",
    "        \n",
    "# Crear DataFrame\n",
    "columns = [\"Base de datos\", \"Resolución (lat, lon)\", \"Rango de latitud\", \"Rango de longitud\",]\n",
    "df_table = pd.DataFrame(table, columns=columns)\n",
    "df_table = df_table.set_index(\"Base de datos\")\n",
    "\n",
    "\n",
    "# Hacemos bonita la tabla\n",
    "df_table[\"Rango de latitud\"] = df_table[\"Rango de latitud\"].apply(lambda x: f\"({round(float(x[0]), 2)}°, {round(float(x[1]), 2)}°)\")\n",
    "df_table[\"Rango de longitud\"] = df_table[\"Rango de longitud\"].apply( lambda x: f\"({round(float(x[0]), 2)}°, {round(float(x[1]), 2)}°)\")\n",
    "df_table[\"Resolución (lat, lon)\"] = df_table[\"Resolución (lat, lon)\"].apply(lambda x: f\"({round(float(x[0]), 3)}°, {round(float(x[1]), 3)}°)\")\n",
    "\n",
    "print(tabulate(df_table, headers=\"keys\", tablefmt=\"pretty\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435ef7ad-4967-408e-87f1-cffa1ab244fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entendemos nuestros datos:\n",
    "sst_a = nc1.variables['sst'][:732] #HadISST: R=1°x1°,\n",
    "sst_b = nc2.variables['sst'][:732] #ERSSTD:  R=2°x2°   \n",
    "pcp = nc3.variables['precip'][:732] #NOAA PCP:  R=2.5°x2.5°\n",
    "pcp2 = nc4.variables['prate'][:732] #NCEP-NCAR:  R=1.889°x1.875°\n",
    "\n",
    "\n",
    "def lat_lon(file_path): #funcion para retornar long y lat en funcion de la data\n",
    "    nc = ncread(file_path, \"r\")\n",
    "    lat_name = next(var for var in nc.variables.keys() if \"lat\" in var.lower()) #busca el nombre asociado las coordenadas\n",
    "    lon_name = next(var for var in nc.variables.keys() if \"lon\" in var.lower()) #suponemos que contiene lat/lon\n",
    "    longitudes = nc.variables[lon_name][:]\n",
    "    latitudes = nc.variables[lat_name][:]                                        \n",
    "    nc.close()\n",
    "    return longitudes, latitudes\n",
    "    \n",
    "lon_a, lat_a  = lat_lon(data1)\n",
    "lon_b, lat_b  = lat_lon(data2)\n",
    "lon_pcp, lat_pcp  = lat_lon(data3)\n",
    "lon_pcp2, lat_pcp2  = lat_lon(data4)\n",
    "\n",
    "nt = len(sst_a[:,0,0])\n",
    "nyr = int(nt/12) \n",
    "ny_a = len(lat_a)\n",
    "ni_a = len(lon_a)   # al haber diferentes resoluciones el numeros de lat y longitud varia asi que mejor mirar todas\n",
    "ny_b = len(lat_b) \n",
    "ni_b = len(lon_b)\n",
    "ny_pcp = len(lat_pcp)\n",
    "ni_pcp = len(lon_pcp)\n",
    "ny_pcp2 = len(lat_pcp2)\n",
    "ni_pcp2 = len(lon_pcp2)\n",
    "\n",
    "sst_a = np.where(sst_a <= -1.79, np.nan, sst_a)  # SST oceánica no baja de -1.8°C\n",
    "sst_a = np.where(sst_a > 40, np.nan, sst_a)    # SST oceánica no sube de ~35°C\n",
    "sst_b = np.where(sst_b <= -1.79, np.nan, sst_b)  # SST oceánica no baja de -1.8°C\n",
    "sst_b = np.where(sst_b > 40, np.nan, sst_b)    # SST oceánica no sube de ~35°C\n",
    "\n",
    "# Calcular medias anuales (732 meses = 61 años)\n",
    "sst_a_annual = sst_a.reshape(61, 12, *sst_a.shape[1:]).mean(axis=1)\n",
    "sst_b_annual = sst_b.reshape(61, 12, *sst_b.shape[1:]).mean(axis=1)\n",
    "\n",
    "Temp_ATL3_a = []\n",
    "Temp_ATL3_b = []\n",
    "Temp_Niño_a = []\n",
    "Temp_Niño_b = []\n",
    "\n",
    "for i in range(61):\n",
    "    matriuATL3_a = sst_a_annual[i, 86:94, 160:180]      # HadISST\n",
    "    Temp_ATL3_a.append(np.mean(matriuATL3_a))\n",
    "\n",
    "    matriuATL3_b = sst_b_annual[i, 42:47, 170:180]      # ERSST\n",
    "    Temp_ATL3_b.append(np.mean(matriuATL3_b))\n",
    "\n",
    "    matriuNiño_a = sst_a_annual[i, 84:96, 10:60]      # HadISST\n",
    "    Temp_Niño_a.append(np.mean(matriuNiño_a))\n",
    "\n",
    "    matriuNiño_b = sst_b_annual[i, 41:47, 95:120]      # ERSST\n",
    "    Temp_Niño_b.append(np.mean(matriuNiño_b))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "Temp_ATL3_a = np.array(Temp_ATL3_a)\n",
    "Temp_ATL3_b = np.array(Temp_ATL3_b)\n",
    "Temp_Niño_a = np.array(Temp_Niño_a)\n",
    "Temp_Niño_b = np.array(Temp_Niño_b)\n",
    "\n",
    "# Tendencias lineales\n",
    "years=(np.arange(1960, 2021, 1))\n",
    "# ATL3\n",
    "coef_ATL3_a = np.polyfit(years, Temp_ATL3_a, 1)\n",
    "coef_ATL3_b = np.polyfit(years, Temp_ATL3_b, 1)\n",
    "\n",
    "trend_ATL3_a = np.polyval(coef_ATL3_a, years)\n",
    "trend_ATL3_b = np.polyval(coef_ATL3_b, years)\n",
    "\n",
    "slope_ATL3_a_dec = coef_ATL3_a[0] * 10\n",
    "slope_ATL3_b_dec = coef_ATL3_b[0] * 10\n",
    "\n",
    "# Niño 3.4\n",
    "coef_Niño_a = np.polyfit(years, Temp_Niño_a, 1)\n",
    "coef_Niño_b = np.polyfit(years, Temp_Niño_b, 1)\n",
    "\n",
    "trend_Niño_a = np.polyval(coef_Niño_a, years)\n",
    "trend_Niño_b = np.polyval(coef_Niño_b, years)\n",
    "\n",
    "slope_Niño_a_dec = coef_Niño_a[0] * 10\n",
    "slope_Niño_b_dec = coef_Niño_b[0] * 10\n",
    "\n",
    "# Figura con dos paneles\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 5), sharex=True)\n",
    "\n",
    "# ATL3\n",
    "axs[0].plot(years, Temp_ATL3_a, 'o-', color='C0', alpha=0.4, label='HadISST (datos)')\n",
    "axs[0].plot(years, Temp_ATL3_b, 'o-', color='C1', alpha=0.4, label='ERSST (datos)')\n",
    "axs[0].plot(years, trend_ATL3_a, color='C0', lw=2.5, label='HadISST – Tendencia')\n",
    "axs[0].plot(years, trend_ATL3_b, color='C1', lw=2.5, label='ERSST – Tendencia')\n",
    "\n",
    "axs[0].set_title('Atlantic Niño (ATL3)')\n",
    "axs[0].set_ylabel('SST Media Anual (°C)')\n",
    "axs[0].set_xlim(1960, 2020)\n",
    "axs[0].grid(True, alpha=0.3)\n",
    "\n",
    "axs[0].text(\n",
    "    0.03, 0.95,\n",
    "    f\"HadISST: {slope_ATL3_a_dec:.2f} °C/década\\n\"\n",
    "    f\"ERSST:  {slope_ATL3_b_dec:.2f} °C/década\",\n",
    "    transform=axs[0].transAxes,\n",
    "    fontsize=10,\n",
    "    verticalalignment='top'\n",
    ")\n",
    "\n",
    "# Niño 3.4\n",
    "axs[1].plot(years, Temp_Niño_a, 'o-', color='C0', alpha=0.4, label='HadISST (datos)')\n",
    "axs[1].plot(years, Temp_Niño_b, 'o-', color='C1', alpha=0.4, label='ERSST (datos)')\n",
    "axs[1].plot(years, trend_Niño_a, color='C0', lw=2.5, label='HadISST – Tendencia')\n",
    "axs[1].plot(years, trend_Niño_b, color='C1', lw=2.5, label='ERSST – Tendencia')\n",
    "\n",
    "axs[1].set_title('Pacific Niño (Niño 3.4)')\n",
    "axs[1].set_xlim(1960, 2020)\n",
    "axs[1].grid(True, alpha=0.3)\n",
    "\n",
    "axs[1].text(\n",
    "    0.03, 0.95,\n",
    "    f\"HadISST: {slope_Niño_a_dec:.2f} °C/década\\n\"\n",
    "    f\"ERSST:  {slope_Niño_b_dec:.2f} °C/década\",\n",
    "    transform=axs[1].transAxes,\n",
    "    fontsize=10,\n",
    "    verticalalignment='top'\n",
    ")\n",
    "# Ejes comunes\n",
    "for ax in axs:\n",
    "    ax.set_xlabel('Años (1960–2020)')\n",
    "    ax.set_xticks(np.arange(1960, 2021, 5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99a620c-9844-4f7a-b548-e5988d45a625",
   "metadata": {},
   "source": [
    "![SST_mean](img/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a58cff-d7d0-4acb-9b4d-391fca8bd01e",
   "metadata": {},
   "source": [
    "$$\n",
    "\\overline{T}\n",
    "=\n",
    "\\frac{\\sum_{\\varphi,\\lambda} T(\\varphi,\\lambda)\\,\\cos(\\varphi)}\n",
    "     {\\sum_{\\varphi,\\lambda} \\cos(\\varphi)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151db975-cc38-4b31-b37e-74855ffa74b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lon_to_180(lon):\n",
    "    lon = np.array(lon, dtype=float)\n",
    "    return ((lon + 180) % 360) - 180\n",
    "\n",
    "def _ensure_increasing(lat, lon, field2d):\n",
    "    lat = np.array(lat); lon = np.array(lon)\n",
    "    if lat[0] > lat[-1]:\n",
    "        lat = lat[::-1]\n",
    "        field2d = field2d[::-1, :]\n",
    "    lon_sort_idx = np.argsort(lon)\n",
    "    lon = lon[lon_sort_idx]\n",
    "    field2d = field2d[:, lon_sort_idx]\n",
    "    return lat, lon, field2d\n",
    "\n",
    "def interp2d_to_target(lat_src, lon_src, field2d, lat_t, lon_t):\n",
    "    lat_src, lon_src, field2d = _ensure_increasing(lat_src, lon_src, field2d)\n",
    "    interp = RegularGridInterpolator((lat_src, lon_src), field2d, method=\"linear\",\n",
    "                                    bounds_error=False, fill_value=np.nan)\n",
    "    LonT, LatT = np.meshgrid(lon_t, lat_t)\n",
    "    pts = np.column_stack([LatT.ravel(), LonT.ravel()])\n",
    "    return interp(pts).reshape(len(lat_t), len(lon_t))\n",
    "\n",
    "def area_weighted_mean(field2d, lat_t):\n",
    "    w = np.cos(np.deg2rad(lat_t))[:, None]\n",
    "    m = np.isfinite(field2d)\n",
    "    return np.nan if not np.any(m) else np.nansum(field2d * w) / np.nansum(w * m)\n",
    "\n",
    "# --- Longitudes coherentes ---\n",
    "lon_a_180 = lon_to_180(lon_a)\n",
    "lon_b_180 = lon_to_180(lon_b)\n",
    "\n",
    "years = np.arange(1960, 2021, 1)\n",
    "ddeg = 0.25\n",
    "\n",
    "def series_box(lat_min, lat_max, lon_min, lon_max):\n",
    "    lat_t = np.arange(lat_min, lat_max + 1e-9, ddeg)\n",
    "    lon_t = np.arange(lon_min, lon_max + 1e-9, ddeg)\n",
    "    A = np.empty(61); B = np.empty(61)\n",
    "    for i in range(61):\n",
    "        fld_a = interp2d_to_target(lat_a, lon_a_180, sst_a_annual[i, :, :], lat_t, lon_t)\n",
    "        fld_b = interp2d_to_target(lat_b, lon_b_180, sst_b_annual[i, :, :], lat_t, lon_t)\n",
    "        A[i] = area_weighted_mean(fld_a, lat_t)\n",
    "        B[i] = area_weighted_mean(fld_b, lat_t)\n",
    "    return A, B\n",
    "\n",
    "def trend(y):\n",
    "    c = np.polyfit(years, y, 1)\n",
    "    return np.polyval(c, years), c[0] * 10\n",
    "\n",
    "# ATL3: 4S–4N, 20W–0  (en -180..180)\n",
    "Temp_ATL3_a_intr, Temp_ATL3_b_intr = series_box(-4, 4, -20, 0)\n",
    "\n",
    "# Niño 3.4 (tus coordenadas): 6S–6N, 170W–120W  -> -170 .. -120\n",
    "Temp_Niño_a_intr, Temp_Niño_b_intr = series_box(-6, 6, -170, -120)\n",
    "\n",
    "trend_ATL3_a, slope_ATL3_a_dec = trend(Temp_ATL3_a_intr)\n",
    "trend_ATL3_b, slope_ATL3_b_dec = trend(Temp_ATL3_b_intr)\n",
    "trend_Niño_a, slope_Niño_a_dec = trend(Temp_Niño_a_intr)\n",
    "trend_Niño_b, slope_Niño_b_dec = trend(Temp_Niño_b_intr)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 5), sharex=True)\n",
    "\n",
    "# --- ATL3 ---\n",
    "axs[0].plot(years, Temp_ATL3_a_intr, 'o-', label='HadISST interp (datos)', color='C0', alpha=0.4)\n",
    "axs[0].plot(years, Temp_ATL3_b_intr, 'o-', label='ERSST interp (datos)', color='C1', alpha=0.4)\n",
    "axs[0].plot(years, trend_ATL3_a, label='HadISST interp – Tendencia', color='C0', lw=2.5)\n",
    "axs[0].plot(years, trend_ATL3_b, label='ERSST interp – Tendencia', color='C1', lw=2.5)\n",
    "axs[0].set_title('Atlantic Niño (ATL3) – Interpolado')\n",
    "axs[0].set_ylabel('SST Media Anual (°C)')\n",
    "axs[0].grid(True, alpha=0.3)\n",
    "axs[0].text(0.02, 0.95,\n",
    "            f\"HadISST: {slope_ATL3_a_dec:.2f} °C/década\\nERSST:  {slope_ATL3_b_dec:.2f} °C/década\",\n",
    "            transform=axs[0].transAxes, fontsize=10, va=\"top\")\n",
    "\n",
    "# --- Niño 3.4 ---\n",
    "axs[1].plot(years, Temp_Niño_a_intr, 'o-', label='HadISST interp (datos)', color='C0', alpha=0.4)\n",
    "axs[1].plot(years, Temp_Niño_b_intr, 'o-', label='ERSST interp (datos)', color='C1', alpha=0.4)\n",
    "axs[1].plot(years, trend_Niño_a, label='HadISST interp – Tendencia', color='C0', lw=2.5)\n",
    "axs[1].plot(years, trend_Niño_b, label='ERSST interp – Tendencia', color='C1', lw=2.5)\n",
    "axs[1].set_title('Pacific Niño (Niño 3.4) – Interpolado')\n",
    "axs[1].grid(True, alpha=0.3)\n",
    "axs[1].text(0.02, 0.95,\n",
    "            f\"HadISST: {slope_Niño_a_dec:.2f} °C/década\\nERSST:  {slope_Niño_b_dec:.2f} °C/década\",\n",
    "            transform=axs[1].transAxes, fontsize=10, va=\"top\")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xlabel('Años (1960–2020)')\n",
    "    ax.set_xlim(1960, 2020)\n",
    "    ax.set_xticks(np.arange(1960, 2021, 5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9968c11-cf45-4b7f-9b2f-7a2c4d6cdfd5",
   "metadata": {},
   "source": [
    "![SST_mean_interpoled](img/2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22929a86-fd0a-4dc2-95f0-47dd38892e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefinimos \n",
    "sst_a = nc1.variables['sst'][:732] #HadISST: R=1°x1°\n",
    "sst_b = nc2.variables['sst'][:732] #ERSST:  R=2°x2°   \n",
    "\n",
    "def to_float_nan(a):\n",
    "    \"\"\"Convierte Variable/MaskedArray a ndarray float con NaNs donde haya máscara.\"\"\"\n",
    "    if np.ma.isMaskedArray(a):\n",
    "        return np.ma.filled(a, np.nan).astype(float)\n",
    "    return np.array(a, dtype=float)\n",
    "\n",
    "def calc_monthly_anoms(fen, detrend=True):\n",
    "    \"\"\"\n",
    "    fen: (time, lat, lon) mensual\n",
    "    return: anoms (year, month, lat, lon)\n",
    "    \"\"\"\n",
    "    arr = to_float_nan(fen)\n",
    "\n",
    "    # Si viniera 4D (time, level, lat, lon), toma level 0\n",
    "    if arr.ndim == 4:\n",
    "        arr = arr[:, 0, :, :]\n",
    "    if arr.ndim != 3:\n",
    "        raise ValueError(f\"Se esperaba (time, lat, lon), pero llegó {arr.shape}\")\n",
    "\n",
    "    nt, ny, nx = arr.shape\n",
    "    nyr = nt // 12\n",
    "    nt_use = nyr * 12\n",
    "    dat = arr[:nt_use].copy()\n",
    "\n",
    "    # Detrend (estable): ajusta con NaNs rellenados, resta solo donde hay dato\n",
    "    if detrend:\n",
    "        x = np.arange(nt_use)\n",
    "        X = np.column_stack([x, np.ones(nt_use)])          # (nt, 2)\n",
    "        Y = dat.reshape(nt_use, -1)                        # (nt, ngrid)\n",
    "        Yf = np.nan_to_num(Y, nan=0.0)                     # para que lstsq no reviente\n",
    "\n",
    "        coeffs = np.linalg.lstsq(X, Yf, rcond=None)[0]     # (2, ngrid)\n",
    "        trend = (X @ coeffs).reshape(dat.shape)            # (nt, ny, nx)\n",
    "\n",
    "        mask = np.isfinite(dat)\n",
    "        dat[mask] = dat[mask] - trend[mask]\n",
    "\n",
    "    # Climatología mensual y anomalías\n",
    "    dat4 = dat.reshape(nyr, 12, ny, nx)                    # (year, month, lat, lon)\n",
    "    clim = np.nanmean(dat4, axis=0)                        # (month, lat, lon)\n",
    "    anoms = dat4 - clim[None, :, :, :]                     # (year, month, lat, lon)\n",
    "    return anoms\n",
    "            \n",
    "#calculem les anomalies dels 4 casos, fem reshape\n",
    "\n",
    "anoms_a0 = calc_monthly_anoms(sst_a, detrend=True)\n",
    "anoms_b = calc_monthly_anoms(sst_b, detrend=True)\n",
    "#en la primera tabla y leyendo la data nos damos cuenta que pcp ya viene en forma de anomalias pero hay que modificar us forma (messes,lat,lon)->(year, month, lat, lon)\n",
    "mes, lat, lon = pcp.shape\n",
    "yr = mes // 12\n",
    "anoms_pcp = pcp.reshape(yr,12,lat,lon)\n",
    "anoms_pcp2 = calc_monthly_anoms(pcp2, detrend=True) #recordar que pcp eran anomalias solo necesitamos calcular las de pcp2\n",
    "\n",
    "def plot_anom_summary(ax, lon, lat, anoms, title, cb_label):\n",
    "    # Campo diagnóstico: máximo absoluto\n",
    "    field = np.nanmax(np.abs(anoms), axis=(0, 1))  # (lat, lon)\n",
    "\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.LAND, facecolor='lightgray')\n",
    "    ax.gridlines(draw_labels=True)\n",
    "\n",
    "    levels = np.linspace(0, 10, 11)\n",
    "\n",
    "    fill = ax.contourf(\n",
    "        lon, lat, field,\n",
    "        levels=levels,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        cmap='YlOrRd',\n",
    "        extend='max'\n",
    "    )\n",
    "\n",
    "    ax.set_title(title, fontsize=12)\n",
    "\n",
    "    cb = plt.colorbar(\n",
    "        fill,\n",
    "        ax=ax,\n",
    "        orientation='horizontal',\n",
    "        pad=0.05,\n",
    "        aspect=40\n",
    "    )\n",
    "    cb.set_label(cb_label, fontsize=12)\n",
    "\n",
    "    # --- mini leyenda min / max ---\n",
    "    vmin = np.nanmin(field)\n",
    "    vmax = np.nanmax(field)\n",
    "\n",
    "    txt = f\"min = {vmin:.2f}\\nmax = {vmax:.2f}\"\n",
    "\n",
    "    ax.text(\n",
    "        0.02, 0.02, txt,\n",
    "        transform=ax.transAxes,\n",
    "        fontsize=9,\n",
    "        verticalalignment='bottom',\n",
    "        horizontalalignment='left',\n",
    "        bbox=dict(\n",
    "            facecolor='white',\n",
    "            edgecolor='none',\n",
    "            alpha=0.75\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return fill\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e88688-45d5-4876-a466-946ecdf3865e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la figura 2×2 para los 4 datasets\n",
    "fig, axes = plt.subplots(\n",
    "    2, 2,\n",
    "    figsize=(16, 12),\n",
    "    subplot_kw={'projection': ccrs.PlateCarree(central_longitude=330)}\n",
    ")\n",
    "\n",
    "plot_anom_summary(\n",
    "    axes[0,0], lon_a, lat_a,\n",
    "    anoms_a0,\n",
    "    'HadISST anomalies: max(|anom|) over 61y×12m',\n",
    "    cb_label='SST anomaly (°C)')\n",
    "\n",
    "plot_anom_summary(\n",
    "    axes[0,1], lon_b, lat_b,\n",
    "    anoms_b,\n",
    "    'ERSST anomalies: max(|anom|) over 61y×12m',\n",
    "    cb_label='SST anomaly (°C)')\n",
    "\n",
    "plot_anom_summary(\n",
    "    axes[1,0], lon_pcp, lat_pcp,\n",
    "    anoms_pcp,\n",
    "    'PCP anomalies: max(|anom|) over 61y×12m',\n",
    "    cb_label='PCP anomaly')\n",
    "\n",
    "plot_anom_summary(\n",
    "    axes[1,1], lon_pcp2, lat_pcp2,\n",
    "    anoms_pcp2,\n",
    "    'PCP2 anomalies: max(|anom|) over 61y×12m',\n",
    "    cb_label='PCP2 anomaly')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda3712b-b7be-4870-a9d2-dc0e6840bcd9",
   "metadata": {},
   "source": [
    "![Data anomalies max](img/3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121b8fcc-ca8e-43f5-bf2c-a7ed4bd318ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "anoms_a = anoms_a0.copy()\n",
    "bad_phys = np.abs(anoms_a0) > 10\n",
    "anoms_a[bad_phys] = np.nan #eliminamos los valores que de desvian muchissimo de lo esperado para evitar outliers en anomalias, y con esto tenemos las anomalias limpias\n",
    "\n",
    "# frecuencia absoluta: número de meses problemáticos\n",
    "thr = 10\n",
    "freq = np.sum(np.abs(anoms_a0) > thr, axis=(0, 1))  # (lat, lon)\n",
    "n_total = anoms_a0.shape[0] * anoms_a0.shape[1]  # 61*12 = 732\n",
    "freq_rel = freq / n_total\n",
    "\n",
    "freq_c = np.sum(np.abs(anoms_a) > thr, axis=(0, 1))  # (lat, lon)\n",
    "n_total = anoms_a.shape[0] * anoms_a.shape[1]  # 61*12 = 732\n",
    "freq_rel_c = freq_c / n_total\n",
    "\n",
    "def plot_freq(ax, lon, lat, freq_field, title, vmax=None):\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.LAND, facecolor='lightgray')\n",
    "    ax.gridlines(draw_labels=True)\n",
    "\n",
    "    if vmax is None:\n",
    "        vmax = np.nanmax(freq_field)\n",
    "    levels = np.linspace(0, vmax, 11)\n",
    "\n",
    "    fill = ax.contourf(\n",
    "        lon, lat, freq_field,\n",
    "        levels=levels,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        cmap='Reds',\n",
    "        extend='max'\n",
    "    )\n",
    "\n",
    "    ax.set_title(title, fontsize=12)\n",
    "\n",
    "    cb = plt.colorbar(\n",
    "        fill,\n",
    "        ax=ax,\n",
    "        orientation='horizontal',\n",
    "        pad=0.05,\n",
    "        aspect=40\n",
    "    )\n",
    "    cb.set_label('Number of months', fontsize=12)\n",
    "\n",
    "    # mini-leyenda min/max\n",
    "    vmin = np.nanmin(freq_field)\n",
    "    vmax_f = np.nanmax(freq_field)\n",
    "    n_total = 61 * 12  # meses\n",
    "    freq_rel_global = np.nanmean(freq_field) / n_total\n",
    "\n",
    "    txt = (\n",
    "        f\"min = {vmin:.0f}\\n\"\n",
    "        f\"max = {vmax_f:.0f}\\n\"\n",
    "        f\"mean freq = {100*freq_rel_global:.2f}%\"\n",
    "    )\n",
    "    ax.text(\n",
    "        0.02, 0.02, txt,\n",
    "        transform=ax.transAxes,\n",
    "        fontsize=9,\n",
    "        va='bottom', ha='left',\n",
    "        bbox=dict(facecolor='white', edgecolor='none', alpha=0.75)\n",
    "    )\n",
    "\n",
    "    return fill\n",
    "\n",
    "    # Escala común para comparar\n",
    "vmax_common = np.nanpercentile(np.maximum(freq, freq_c), 99)\n",
    "if vmax_common < 1:\n",
    "    vmax_common = np.nanmax(np.maximum(freq, freq_c))\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    1, 2,\n",
    "    figsize=(16, 5),\n",
    "    subplot_kw={'projection': ccrs.PlateCarree(central_longitude=330)}\n",
    ")\n",
    "\n",
    "plot_freq(\n",
    "    axes[0], lon_a, lat_a,\n",
    "    freq,\n",
    "    f'HadISST frequency: |anom| > {thr}°C (raw)',\n",
    "    vmax=vmax_common\n",
    ")\n",
    "\n",
    "plot_freq(\n",
    "    axes[1], lon_a, lat_a,\n",
    "    freq_c,\n",
    "    f'HadISST frequency: |anom| > {thr}°C (cleaned)',\n",
    "    vmax=vmax_common\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8331222c-ccc0-4f0e-84ab-aad976805404",
   "metadata": {},
   "source": [
    "![Clean Outliers Haddist](img/4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee7ea1c-f8f5-4c37-8e94-9e0a82d20e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_mensual(var_mon, lat_src, lon_src, lat_min, lat_max, lon_min, lon_max,\n",
    "                      ddeg=0.25, reducer=\"area_mean\"):\n",
    "    lon_src_180 = lon_to_180(lon_src)\n",
    "\n",
    "    lat_t = np.arange(lat_min, lat_max + 1e-9, ddeg)\n",
    "    lon_t = np.arange(lon_min, lon_max + 1e-9, ddeg)\n",
    "\n",
    "    out = np.empty(12, dtype=float)\n",
    "\n",
    "    for m in range(12):\n",
    "        fld = interp2d_to_target(lat_src, lon_src_180, var_mon[m, :, :], lat_t, lon_t)\n",
    "\n",
    "        if reducer == \"area_mean\":\n",
    "            out[m] = area_weighted_mean(fld, lat_t)\n",
    "        elif reducer == \"mean\":\n",
    "            out[m] = np.nanmean(fld)\n",
    "        else:\n",
    "            raise ValueError(\"reducer must be 'area_mean' or 'mean'\")\n",
    "\n",
    "    return out\n",
    "\n",
    "# std mensual ya calculada\n",
    "var_a = np.nanstd(anoms_a, axis=0)      # (12, lat, lon)\n",
    "var_b = np.nanstd(anoms_b, axis=0)\n",
    "var_pcp = np.nanstd(anoms_pcp, axis=0)\n",
    "var_pcp2 = np.nanstd(anoms_pcp2, axis=0)\n",
    "\n",
    "variacion_mensual_ATL3_a  = var_mensual(var_a, lat_a, lon_a, -4,  4,  -20,    0, ddeg=0.25)\n",
    "variacion_mensual_Niño_a  = var_mensual(var_a, lat_a, lon_a, -6,  6, -170, -120, ddeg=0.25)\n",
    "\n",
    "variacion_mensual_ATL3_b  = var_mensual(var_b, lat_b, lon_b, -4,  4,  -20,    0, ddeg=0.25)\n",
    "variacion_mensual_Niño_b  = var_mensual(var_b, lat_b, lon_b, -6,  6, -170, -120, ddeg=0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5d16c9-9642-4ed8-86ae-17977987d5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar el gráfico\n",
    "\n",
    "meses = ['Jan','Feb','Mar','Abr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\n",
    "\n",
    "\n",
    "# SSTxATL3 y STTxNiño3.4\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "axs[0].plot(meses, variacion_mensual_ATL3_a,'-ro', markersize=2)\n",
    "axs[0].plot(meses, variacion_mensual_ATL3_b,'-bo', markersize=2)\n",
    "axs[0].grid() # Agrega un fondo cuadriculado al gráfico\n",
    "axs[0].set_ylabel('Variability (°C)') # Renombra el eje y como \"Variability (°C)\"\n",
    "axs[0].set_title('ATL3') \n",
    "axs[0].set_yticks(np.linspace(0.3, 0.7, 12)) # Establece 9 divisiones en el eje y\n",
    "axs[0].yaxis.set_major_formatter(ticker.FormatStrFormatter('%.3f')) # Formatea el eje y para mostrar solo 3 decimales\n",
    "axs[0].margins(x=0.0909)\n",
    "axs[0].legend(['HadISST','ERSST'])\n",
    "\n",
    "\n",
    "axs[1].plot(meses, variacion_mensual_Niño_a,'-ro', markersize=2)\n",
    "axs[1].plot(meses, variacion_mensual_Niño_b,'-bo', markersize=2)\n",
    "axs[1].grid() # Agrega un fondo cuadriculado al gráfico\n",
    "axs[1].set_ylabel('Variability (°C)') # Renombra el eje y como \"Variability (°C)\"\n",
    "axs[1].set_title('Niño3.4') \n",
    "axs[1].set_yticks(np.linspace(0.5, 1.2, 12)) # Establece 9 divisiones en el eje y\n",
    "axs[1].yaxis.set_major_formatter(ticker.FormatStrFormatter('%.3f')) # Formatea el eje y para mostrar solo 3 decimales\n",
    "axs[1].margins(x=0.0909)\n",
    "axs[1].legend(['HadISST','ERSST'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ed209b-40ee-4ab1-883a-ebaba217514b",
   "metadata": {},
   "source": [
    "![Variabilidad Anom_SST](img/5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7d8e7c-dfcf-471e-9bde-53ec0d0516e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "variacion_mensual_ATL3_pcp  = var_mensual(var_pcp, lat_pcp, lon_pcp, -4,  4,  -20,    0, ddeg=0.25)\n",
    "variacion_mensual_Niño_pcp  = var_mensual(var_pcp, lat_pcp, lon_pcp, -6,  6, -170, -120, ddeg=0.25)\n",
    "\n",
    "variacion_mensual_ATL3_pcp2  = var_mensual(var_pcp2, lat_pcp2, lon_pcp2, -4,  4,  -20,    0, ddeg=0.25)\n",
    "variacion_mensual_Niño_pcp2  = var_mensual(var_pcp2, lat_pcp2, lon_pcp2, -6,  6, -170, -120, ddeg=0.25)\n",
    "\n",
    "# SSTxATL3 y STTxNiño3.4\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "axs[0].plot(meses, variacion_mensual_ATL3_pcp,'-ro', markersize=2)\n",
    "axs[0].plot(meses, variacion_mensual_ATL3_pcp2,'-bo', markersize=2)\n",
    "axs[0].grid() # Agrega un fondo cuadriculado al gráfico\n",
    "axs[0].set_ylabel('Variability (mm/day)') # Renombra el eje y como \"Variability (°C)\"\n",
    "axs[0].set_title('ATL3') \n",
    "axs[0].set_yticks(np.linspace(0, 2, 12)) # Establece 9 divisiones en el eje y\n",
    "axs[0].yaxis.set_major_formatter(ticker.FormatStrFormatter('%.3f')) # Formatea el eje y para mostrar solo 3 decimales\n",
    "axs[0].margins(x=0.0909)\n",
    "axs[0].legend(['NOAA PCP','NCEP-NCAR'])\n",
    "\n",
    "\n",
    "axs[1].plot(meses, variacion_mensual_Niño_pcp,'-ro', markersize=2)\n",
    "axs[1].plot(meses, variacion_mensual_Niño_pcp2,'-bo', markersize=2)\n",
    "axs[1].grid() # Agrega un fondo cuadriculado al gráfico\n",
    "axs[1].set_ylabel('Variability (mm/day)') # Renombra el eje y como \"Variability (°C)\"\n",
    "axs[1].set_title('Niño3.4') \n",
    "axs[1].set_yticks(np.linspace(0.5, 3, 12)) # Establece 9 divisiones en el eje y\n",
    "axs[1].yaxis.set_major_formatter(ticker.FormatStrFormatter('%.3f')) # Formatea el eje y para mostrar solo 3 decimales\n",
    "axs[1].margins(x=0.0909)\n",
    "axs[1].legend(['NOAA PCP','NCEP-NCAR'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d601288-0b47-4016-8701-ea190646636d",
   "metadata": {},
   "source": [
    "![Variabilidad Anom_PCP](img/6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603b548d-575f-4170-a9dd-6af2319d3891",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a definir los indices globales como las anomalias entre la variabilidad\n",
    "index_sst_a = anoms_a / var_a        #HadISST\n",
    "index_sst_b = anoms_b / var_b        #ERSST\n",
    "index_pcp = anoms_pcp / var_pcp      #NOAA PCP\n",
    "index_pcp2 = anoms_pcp2 / var_pcp2   #NCEP-NCAR \n",
    "\n",
    "#A partir de estos indices vamos a calcular su media temporal en los meses de mayor intensidad de cada evento, tendremos como resultado matrices 2D delimitadas por las zonas\n",
    "JJ_months = [5,6]\n",
    "ND_months = [10, 11]\n",
    "\n",
    "index_sst_a_JJ = np.nanmean(index_sst_a[:, JJ_months, :, :], axis=1)\n",
    "index_sst_a_ND = np.nanmean(index_sst_a[:, ND_months, :, :], axis=1)\n",
    "\n",
    "index_sst_b_JJ = np.nanmean(index_sst_b[:, JJ_months, :, :], axis=1)\n",
    "index_sst_b_ND = np.nanmean(index_sst_b[:, ND_months, :, :], axis=1)\n",
    "\n",
    "index_pcp_JJ = np.nanmean(index_pcp[:, JJ_months, :, :], axis=1)\n",
    "index_pcp_ND = np.nanmean(index_pcp[:, ND_months, :, :], axis=1)\n",
    "\n",
    "index_pcp2_JJ = np.nanmean(index_pcp2[:, JJ_months, :, :], axis=1)\n",
    "index_pcp2_ND = np.nanmean(index_pcp2[:, ND_months, :, :], axis=1)\n",
    "\n",
    "#Tambien vamos a calcular la media de estos indices en la zona de cada evento.\n",
    "\n",
    "# Regiones (en coordenadas físicas)\n",
    "ATL3_lats = (-4.0, 4.0)\n",
    "ATL3_lons = (-20, 0)      # 20W–0E \n",
    "\n",
    "NIÑO_lats = (-6.0, 6.0)\n",
    "NIÑO_lons = (-170, -120)      # 170W–120W \n",
    "\n",
    "\n",
    "def area_mean_index(season_field, lat, lon, lat_bounds, lon_bounds):\n",
    "    lat = np.asarray(lat)\n",
    "    lon = lon_to_180(lon)\n",
    "\n",
    "    lat_min, lat_max = lat_bounds\n",
    "    lon_min, lon_max = lon_bounds\n",
    "\n",
    "    lat_mask = (lat >= lat_min) & (lat <= lat_max)\n",
    "    lon_mask = (lon >= lon_min) & (lon <= lon_max)\n",
    "\n",
    "    sub = season_field[:, lat_mask, :][:, :, lon_mask]\n",
    "    return np.nanmean(sub, axis=(1, 2))\n",
    "    \n",
    "#Vamos a coger los indices en forma de matriz anteriores y les vamos a calcular la media de temperatura para cada zona especifica, este indice resultantante solo tendra dimension temporal\n",
    "# Hadisst\n",
    "ATL3_sst_a_index = area_mean_index(index_sst_a_JJ, lat_a, lon_a, ATL3_lats, ATL3_lons)\n",
    "Niño_sst_a_index = area_mean_index(index_sst_a_ND, lat_a, lon_a, NIÑO_lats, NIÑO_lons) \n",
    "\n",
    "# ERSST\n",
    "ATL3_sst_b_index = area_mean_index(index_sst_b_JJ, lat_b, lon_b, ATL3_lats, ATL3_lons)\n",
    "Niño_sst_b_index = area_mean_index(index_sst_b_ND, lat_b, lon_b, NIÑO_lats, NIÑO_lons)  \n",
    "    \n",
    "# NOAA PCP\n",
    "ATL3_pcp_index = area_mean_index(index_pcp_JJ, lat_pcp, lon_pcp, ATL3_lats, ATL3_lons)\n",
    "Niño_pcp_index = area_mean_index(index_pcp_ND, lat_pcp, lon_pcp, NIÑO_lats, NIÑO_lons) \n",
    "\n",
    "# #NCEP-NCAR\n",
    "ATL3_pcp2_index = area_mean_index(index_pcp2_JJ, lat_pcp2, lon_pcp2, ATL3_lats, ATL3_lons)\n",
    "Niño_pcp2_index = area_mean_index(index_pcp2_ND, lat_pcp2, lon_pcp2, NIÑO_lats, NIÑO_lons)\n",
    "\n",
    "#Una vez tenemos los indices vamos a calcular la media de anomalies en en los messes de Julio Junio y de Noviembre Diciembre, estas son globales\n",
    "anoms_a_JJ = np.mean(anoms_a[:,JJ_months,:,:], axis=1)\n",
    "anoms_b_JJ = np.mean(anoms_b[:,JJ_months,:,:], axis=1)\n",
    "anoms_pcp_JJ = np.mean(anoms_pcp[:,JJ_months,:,:], axis=1)\n",
    "anoms_pcp2_JJ = np.mean(anoms_pcp2[:,JJ_months,:,:], axis=1)\n",
    "\n",
    "anoms_a_ND = np.mean(anoms_a[:,ND_months,:,:], axis=1)\n",
    "anoms_b_ND = np.mean(anoms_b[:,ND_months,:,:], axis=1)\n",
    "anoms_pcp_ND = np.mean(anoms_pcp[:,ND_months,:,:], axis=1)\n",
    "anoms_pcp2_ND = np.mean(anoms_pcp2[:,ND_months,:,:], axis=1)\n",
    "\n",
    "#Vamos a definir una funcion donde poder ver la tendencia y las correlacion entre estas anomalias en los periodos JJ y ND y su indice normalizado y medio de cada zona  \n",
    "# Analizamos en qué regiones del mundo las anomalías de SST o precipitación\n",
    "# covarían con el índice regional (ATL3 o Niño3.4).\n",
    "# Las zonas con alta correlación indican regiones donde la variabilidad\n",
    "# interanual está fuertemente acoplada al índice, por lo tanto al evento climatico\n",
    "\n",
    "def find_a_r(anom_det, index):\n",
    "    index = np.asarray(index)\n",
    "    n_time = len(index)\n",
    "\n",
    "    x = index\n",
    "    x_mean = np.mean(x)\n",
    "    x_centered = x - x_mean\n",
    "\n",
    "    y = anom_det  # (time, lat, lon)\n",
    "    y_mean = np.mean(y, axis=0, keepdims=True)\n",
    "    y_centered = y - y_mean\n",
    "\n",
    "    cov = np.sum(x_centered[:, np.newaxis, np.newaxis] * y_centered, axis=0) / (n_time - 1)\n",
    "    var_x = np.var(x, ddof=1)\n",
    "    slopes = cov / var_x\n",
    "\n",
    "    std_y = np.std(y, axis=0, ddof=1)\n",
    "    correlations = cov / (np.std(x, ddof=1) * std_y)\n",
    "\n",
    "    return slopes, correlations\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3040ffc5-3ed2-4437-a702-30dab235152b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_HADISST_ATL3, corr_HADISST_ATL3 = find_a_r(anoms_a_JJ, ATL3_sst_a_index)\n",
    "\n",
    "a_HADISST_NIÑO, corr_HADISST_NIÑO = find_a_r(anoms_a_ND, Niño_sst_a_index)\n",
    "\n",
    "a_ERSST_ATL3, corr_ERSST_ATL3 = find_a_r(anoms_b_JJ, ATL3_sst_b_index)\n",
    "\n",
    "a_ERSST_NIÑO, corr_ERSST_NIÑO = find_a_r(anoms_b_ND, Niño_sst_b_index)\n",
    "\n",
    "a_pcp_ATL3, corr_pcp_ATL3 = find_a_r(anoms_pcp_JJ, ATL3_pcp_index)\n",
    "\n",
    "a_pcp_NIÑO, corr_pcp_NIÑO = find_a_r(anoms_pcp_ND, Niño_pcp_index)\n",
    "\n",
    "a_pcp2_ATL3, corr_pcp2_ATL3 = find_a_r(anoms_pcp2_JJ, ATL3_pcp2_index)\n",
    "\n",
    "a_pcp2_NIÑO, corr_pcp2_NIÑO = find_a_r(anoms_pcp2_ND, Niño_pcp2_index)\n",
    "\n",
    "def configure_plot(ax, title, cb_label, data_type='temp'):\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.LAND, facecolor='lightgray')\n",
    "    ax.gridlines(draw_labels=True)\n",
    "\n",
    "    cmap = plt.cm.RdBu_r\n",
    "    clev = np.linspace(-1, 1, 11)\n",
    "\n",
    "    return cmap, clev\n",
    "    \n",
    "def create_plot(ax, lon, lat, slope_data, corr_data, title, cb_label, data_type='temp'):\n",
    "    # --- Constantes del estudio ---\n",
    "    N = 61\n",
    "    alpha = 0.05\n",
    "    df = N - 2\n",
    "\n",
    "    # --- Base map ---\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.LAND, facecolor='lightgray')\n",
    "    ax.gridlines(draw_labels=True)\n",
    "\n",
    "    cmap = plt.cm.RdBu_r\n",
    "\n",
    "    slope_plot = np.array(slope_data, copy=True)\n",
    "    corr_plot  = np.array(corr_data,  copy=True)\n",
    "\n",
    "    # COLORES-SLOPE\n",
    "    if data_type == 'temp':\n",
    "        clev_slope = np.linspace(-1, 1, 11)\n",
    "    else:\n",
    "        vmax = np.nanpercentile(np.abs(slope_plot), 95)\n",
    "        vmax = max(min(vmax, 2.0), 1e-6)\n",
    "        clev_slope = np.linspace(-vmax, vmax, 11)\n",
    "\n",
    "    fill = ax.contourf(\n",
    "        lon, lat, slope_plot,\n",
    "        levels=clev_slope,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        cmap=cmap,\n",
    "        extend='both'\n",
    "    )\n",
    "\n",
    "    \n",
    "    #CONTORNOS = r (tamaño de correlación)\n",
    "    corr_levels = [-0.6, -0.4, -0.2, 0.2, 0.4, 0.6]\n",
    "    cont = ax.contour(lon, lat, corr_plot,levels=corr_levels,transform=ccrs.PlateCarree(),colors='k',linewidths=0.7,alpha=0.7)\n",
    "    ax.clabel(cont, inline=True, fontsize=8, fmt=\"%.1f\")\n",
    "\n",
    "    \n",
    "    #HACHURADO = p < 0.05 (significancia de r)\n",
    "    r = np.clip(corr_plot, -0.999999, 0.999999)\n",
    "    t_stat = r * np.sqrt(df / (1.0 - r**2))\n",
    "    pval = 2.0 * stats.t.sf(np.abs(t_stat), df)\n",
    "    sig = (pval < alpha).astype(int)  # 1 significativo\n",
    "    ax.contourf(lon, lat, sig,levels=[0.5, 1.5],transform=ccrs.PlateCarree(),colors='none',hatches=['....'])  # cambia a '///' si lo prefieres)\n",
    "\n",
    "    # --- Título y colorbar ---\n",
    "    ax.set_title(title, fontsize=12)\n",
    "\n",
    "    cb = plt.colorbar(\n",
    "        fill,\n",
    "        ax=ax,\n",
    "        orientation='horizontal',\n",
    "        pad=0.05,\n",
    "        aspect=40\n",
    "    )\n",
    "    cb.set_label(cb_label, fontsize=12)\n",
    "\n",
    "    return fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020de939-8f14-4bd4-abe6-158becebdbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSTxATL3 y STTxNiño3.4\n",
    "fig, axes = plt.subplots(\n",
    "    2, 2,\n",
    "    figsize=(16, 12),\n",
    "    subplot_kw={'projection': ccrs.PlateCarree(central_longitude=330)}\n",
    ")\n",
    "\n",
    "create_plot(\n",
    "    axes[0,0], lon_a, lat_a,\n",
    "    a_HADISST_ATL3, corr_HADISST_ATL3,\n",
    "    'SSTxALT3 JJ (HADISST)', 'Temperature (°C)'\n",
    ")\n",
    "\n",
    "create_plot(\n",
    "    axes[0,1], lon_a, lat_a,\n",
    "    a_HADISST_NIÑO, corr_HADISST_NIÑO,\n",
    "    'SSTxNiño3.4 ND (HADISST)', 'Temperature (°C)'\n",
    ")\n",
    "\n",
    "create_plot(\n",
    "    axes[1,0], lon_b, lat_b,\n",
    "    a_ERSST_ATL3, corr_ERSST_ATL3,\n",
    "    'SSTxALT3 JJ (ERSST)', 'Temperature (°C)'\n",
    ")\n",
    "\n",
    "create_plot(\n",
    "    axes[1,1], lon_b, lat_b,\n",
    "    a_ERSST_NIÑO, corr_ERSST_NIÑO,\n",
    "    'SSTxNiño3.4 ND (ERSST)', 'Temperature (°C)'\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91a1643-8c0b-4c39-969b-a51817a16ff5",
   "metadata": {},
   "source": [
    "![Correlacion/Slope Anom_SST-index](img/7.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624a4443-d718-4e69-9bd7-8977669294ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCPxATL3 y PCPxNiño3.4\n",
    "fig, axes = plt.subplots(\n",
    "    2, 2,\n",
    "    figsize=(16, 12),\n",
    "    subplot_kw={'projection': ccrs.PlateCarree(central_longitude=330)}\n",
    ")\n",
    "\n",
    "create_plot(\n",
    "    axes[0,0], lon_pcp, lat_pcp,\n",
    "    a_pcp_ATL3, corr_pcp_ATL3,\n",
    "    'PCPxALT3 JJ (NOAA)', 'Precipitation (mm/day)', 'precip'\n",
    ")\n",
    "\n",
    "create_plot(\n",
    "    axes[0,1], lon_pcp, lat_pcp,\n",
    "    a_pcp_NIÑO, corr_pcp_NIÑO,\n",
    "    'PCPxNiño3.4 ND (NOAA)', 'Precipitation (mm/day)', 'precip'\n",
    ")\n",
    "\n",
    "create_plot(\n",
    "    axes[1,0], lon_pcp2, lat_pcp2,\n",
    "    a_pcp2_ATL3, corr_pcp2_ATL3,\n",
    "    'PCP2xALT3 JJ (NCEP)', 'Precipitation (mm/day)', 'precip'\n",
    ")\n",
    "\n",
    "create_plot(\n",
    "    axes[1,1], lon_pcp2, lat_pcp2,\n",
    "    a_pcp2_NIÑO, corr_pcp2_NIÑO,\n",
    "    'PCP2xNiño3.4 ND (NCEP)', 'Precipitation (mm/day)', 'precip'\n",
    ")\n",
    " \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde53dcd-6478-49d5-a56b-29d6370b3a48",
   "metadata": {},
   "source": [
    "![Correlacion/Slope Anom_PCP-index](img/8.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642fe7fc-3db1-42a1-9b9d-7043d6e79719",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N = 5\n",
    "\n",
    "def top5(years, a, b, n=5):\n",
    "    comb = (a + b) / 2\n",
    "    idx = np.argsort(comb)[-n:]\n",
    "    df = pd.DataFrame({\"Año\": years[idx], \"HadISST (°C)\": a[idx], \"ERSST (°C)\": b[idx]})\n",
    "    return df.sort_values(\"Año\").set_index(\"Año\")\n",
    "\n",
    "atl3 = top5(years, Temp_ATL3_a, Temp_ATL3_b, N)\n",
    "niño = top5(years, Temp_Niño_a, Temp_Niño_b, N)\n",
    "\n",
    "fmt = {\"HadISST (°C)\": \"{:.2f}\", \"ERSST (°C)\": \"{:.2f}\"}\n",
    "header_style = [{\"selector\":\"th\",\"props\":[(\"padding-top\",\"2px\"),(\"padding-bottom\",\"2px\"),\n",
    "                                         (\"vertical-align\",\"bottom\"),(\"text-align\",\"center\")]}]\n",
    "\n",
    "s_atl3 = atl3.style.format(fmt).set_properties(**{\"text-align\":\"center\"}).set_table_styles(header_style)\n",
    "s_niño = niño.style.format(fmt).set_properties(**{\"text-align\":\"center\"}).set_table_styles(header_style)\n",
    "\n",
    "html = f\"\"\"\n",
    "<div style=\"text-align:left; font-weight:bold; font-size:15px; margin-bottom:12px;\">\n",
    "Picos de SST media anual (1960–2020)\n",
    "</div>\n",
    "\n",
    "<div style=\"display:flex; justify-content:left; gap:80px;\">\n",
    "  <div>\n",
    "    <div style=\"font-weight:bold; margin-bottom:6px;\">Atlantic Niño (ATL3):</div>\n",
    "    {s_atl3.to_html()}\n",
    "  </div>\n",
    "  <div>\n",
    "    <div style=\"font-weight:bold; margin-bottom:6px;\">Pacific Niño (Niño 3.4):</div>\n",
    "    {s_niño.to_html()}\n",
    "  </div>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2324d26-9a54-4948-a957-f5d97ce2af90",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRI_LABELS = [\"DJF\",\"JFM\",\"FMA\",\"MAM\",\"AMJ\",\"MJJ\",\"JJA\",\"JAS\",\"ASO\",\"SON\",\"OND\",\"NDJ\"]\n",
    "\n",
    "# COORDENADAS (Tuplas definidas por el usuario)\n",
    "ATL3_lats = (-4.0, 4.0)\n",
    "ATL3_lons = (-20, 0)        # 20W–0E \n",
    "\n",
    "NIÑO_lats = (-6.0, 6.0)\n",
    "NIÑO_lons = (-170, -120)    # 170W–120W \n",
    "\n",
    "# Umbrales para clasificación Z\n",
    "Z_THR_WEAK = 0.5   \n",
    "Z_THR_MOD  = 1.0\n",
    "Z_THR_STRONG = 1.5\n",
    "\n",
    "\n",
    "# 2. FUNCIONES DE EXTRACCIÓN DE DATOS (\n",
    "\n",
    "\n",
    "def anom_year(year, anoms, year0=1960):\n",
    "    #Extrae anomalías mensuales (12, lat, lon) para un año específico\n",
    "    if year < year0:\n",
    "        raise ValueError(f\"No hay datos anteriores a {year0}\")\n",
    "    i = year - year0\n",
    "    if i < 0 or i >= anoms.shape[0]:\n",
    "        year_last = year0 + anoms.shape[0] - 1\n",
    "        raise ValueError(f\"Año fuera de rango. Disponible: {year0}–{year_last}\")\n",
    "    return anoms[i, :, :, :]\n",
    "\n",
    "def mensual_region(mon_field, lat_src, lon_src, lat_min, lat_max, lon_min, lon_max,\n",
    "                   ddeg=0.25, reducer=\"area_mean\"):\n",
    "\n",
    "    lon_src_180 = lon_to_180(lon_src)\n",
    "    lat_t = np.arange(lat_min, lat_max + 1e-9, ddeg)\n",
    "    lon_t = np.arange(lon_min, lon_max + 1e-9, ddeg)\n",
    "    out = np.empty(12, dtype=float)\n",
    "\n",
    "    for m in range(12):\n",
    "        fld = interp2d_to_target(lat_src, lon_src_180, mon_field[m, :, :], lat_t, lon_t)\n",
    "        if reducer == \"area_mean\":\n",
    "            out[m] = area_weighted_mean(fld, lat_t)\n",
    "        elif reducer == \"mean\":\n",
    "            out[m] = np.nanmean(fld)\n",
    "        else:\n",
    "            raise ValueError(\"reducer must be 'area_mean' or 'mean'\")\n",
    "    return out\n",
    "\n",
    "def niño_monthly_series(year, lat_min_n, lat_max_n, lon_min_n, lon_max_n): #Sirve para tener las anomalias en la region para cada base de datos\n",
    "\n",
    "\n",
    "    anom_a_y = anom_year(year, anoms_a, year0=1960)\n",
    "    anom_b_y = anom_year(year, anoms_b, year0=1960)\n",
    "\n",
    "    niño_a = mensual_region(anom_a_y, lat_a, lon_a, lat_min_n, lat_max_n, lon_min_n, lon_max_n, ddeg=0.25)\n",
    "    niño_b = mensual_region(anom_b_y, lat_b, lon_b, lat_min_n, lat_max_n, lon_min_n, lon_max_n, ddeg=0.25)\n",
    "    return niño_a, niño_b\n",
    "\n",
    "# =============================================================================\n",
    "# 3. FUNCIONES MATEMÁTICAS Y ESTADÍSTICAS (STATS HELPERS)\n",
    "# =============================================================================\n",
    "\n",
    "def runs(mask, n): #rachas\n",
    "    m = np.asarray(mask, dtype=bool)\n",
    "    x = np.r_[False, m, False]\n",
    "    d = np.diff(x.astype(int))\n",
    "    starts, ends = np.where(d == 1)[0], np.where(d == -1)[0]\n",
    "    out = np.zeros_like(m, dtype=bool)\n",
    "    for s, e in zip(starts, ends):\n",
    "        if (e - s) >= n:\n",
    "            out[s:e] = True\n",
    "    return out\n",
    "\n",
    "def three_month_running(series_24, year1, year2): #media movil ONI\n",
    "    s = np.asarray(series_24, dtype=float)\n",
    "    if s.shape[0] != 24: raise ValueError(\"Esperaba 24 meses\")\n",
    "    \n",
    "    tri = np.array([np.nanmean(s[i:i+3]) for i in range(22)])\n",
    "    centers_idx = np.arange(22) + 1\n",
    "    centers_month = centers_idx % 12\n",
    "    \n",
    "    centers_year = np.where(centers_idx < 12, year1, year2)\n",
    "    labs = [TRI_LABELS[m] for m in centers_month]\n",
    "    tri_lab_year = np.array([f\"{l}\\n{y}\" for l, y in zip(labs, centers_year)])\n",
    "    \n",
    "    return tri, tri_lab_year, centers_idx\n",
    "\n",
    "def mark_center_months(valid_tri, centers_idx):\n",
    "    #marca el centro de cada media movil en el grafico mes a mes como punto valido. es algo visual \n",
    "    #antes haciamos las medias sobre ese mes central pero ya no tiene este uso, usamos directamente los meses de picos para eso.\n",
    "    out = np.zeros(24, dtype=bool)\n",
    "    for ok, c in zip(valid_tri, centers_idx):\n",
    "        if ok and 0 <= c < 24: out[c] = True\n",
    "    return out\n",
    "\n",
    "def calculate_seasonal_stats(series, sigma_monthly, months_sel):\n",
    "    \"\"\"Calcula media, sigma combinada y Z.\"\"\"\n",
    "    mean_season = float(np.mean(np.asarray(series)[months_sel]))\n",
    "    sigs = np.asarray(sigma_monthly, dtype=float)[months_sel]\n",
    "    sig_season = float(np.sqrt(np.nanmean(sigs**2)))\n",
    "    \n",
    "    z = mean_season / sig_season if (sig_season > 0 and np.isfinite(mean_season)) else np.nan\n",
    "    return mean_season, sig_season, z\n",
    "\n",
    "def classify_by_z(z, thr_weak=1.0, thr_moderate=1.5, thr_strong=2.0):\n",
    "    if not np.isfinite(z): return \"n/a\"\n",
    "    if z < thr_weak: return \"none\"\n",
    "    if z < thr_moderate: return \"weak\"\n",
    "    if z < thr_strong: return \"moderate\"\n",
    "    return \"strong\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eaa962-9879-4b3f-868f-3fa17e4310b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Anoms_ATL3_JJ_a = []\n",
    "\n",
    "for i in range(61):\n",
    "    # JJ\n",
    "    atl3_JJ = anoms_a[i, 5:7, 86:94, 160:180]  # (3, lat, lon)\n",
    "    Anoms_ATL3_JJ_a.append(np.nanmean(atl3_JJ))\n",
    "\n",
    "Anoms_ATL3_JJ_a = np.array(Anoms_ATL3_JJ_a)\n",
    "\n",
    "Anoms_ATL3_JJ_b = []\n",
    "\n",
    "for i in range(61):\n",
    "    atl3_JJ = anoms_b[i, 5:7, 42:47, 170:180]\n",
    "    Anoms_ATL3_JJ_b.append(np.nanmean(atl3_JJ))\n",
    "\n",
    "Anoms_ATL3_JJ_b = np.array(Anoms_ATL3_JJ_b)\n",
    "\n",
    "\n",
    "#Niño3.4\n",
    "Anoms_Niño3_ND_a = []\n",
    "\n",
    "for i in range(61):\n",
    "    # ND\n",
    "    Niño3_ND = anoms_a[i, 10:12, 84:96, 10:60]  # (3, lat, lon)\n",
    "    Anoms_Niño3_ND_a.append(np.nanmean(Niño3_ND))\n",
    "    \n",
    "Anoms_Niño3_ND_a = np.array(Anoms_Niño3_ND_a)\n",
    "\n",
    "Anoms_Niño3_ND_b = []\n",
    "def plot_hist_gauss_with_z(ax, data, title, bins=10):\n",
    "    data = np.asarray(data, dtype=float)\n",
    "    data = data[np.isfinite(data)]\n",
    "    n = data.size\n",
    "\n",
    "    mu = np.mean(data)\n",
    "    sigma = np.std(data)\n",
    "\n",
    "    # histograma\n",
    "    ax.hist(data, bins=bins, density=True, alpha=0.6)\n",
    "\n",
    "    # gaussiana ajustada\n",
    "    if sigma > 0:\n",
    "        x_min, x_max = np.min(data), np.max(data)\n",
    "        dx = x_max - x_min\n",
    "        x = np.linspace(x_min - 0.15*dx, x_max + 0.15*dx, 400)\n",
    "        pdf = (1/(sigma*np.sqrt(2*np.pi))) * np.exp(-0.5*((x-mu)/sigma)**2)\n",
    "        ax.plot(x, pdf, lw=2)\n",
    "\n",
    "    # percentiles empíricos\n",
    "    p84 = np.percentile(data, 84)\n",
    "    p90 = np.percentile(data, 90)\n",
    "\n",
    "    # valores Z=±1 en unidades de dato\n",
    "    z1_val = mu + sigma\n",
    "    zm1_val = mu - sigma\n",
    "\n",
    "    # percentil empírico asociado a Z=1 (mu+sigma)\n",
    "    z1_pct = 100.0 * np.mean(data <= z1_val)\n",
    "\n",
    "    # líneas guía\n",
    "    ax.axvline(mu, lw=2, linestyle='-')          # media\n",
    "    if sigma > 0:\n",
    "        ax.axvline(zm1_val, lw=2, linestyle='--')  # mu - sigma\n",
    "        ax.axvline(z1_val,  lw=2, linestyle='--')  # mu + sigma\n",
    "\n",
    "    ax.axvline(p84, lw=2, linestyle=':')         # P84\n",
    "    ax.axvline(p90, lw=2, linestyle=':')         # P90\n",
    "\n",
    "    # anotación compacta (incluye Z y percentil)\n",
    "    txt = (\n",
    "        f\"N = {n}\\n\"\n",
    "        f\"μ = {mu:.3f}\\n\"\n",
    "        f\"σ = {sigma:.3f}\\n\"\n",
    "        f\"Z=1 → μ+σ = {z1_val:.3f}\\n\"\n",
    "        f\"Percentil(μ+σ) = {z1_pct:.1f}\\n\"\n",
    "        f\"P84 = {p84:.3f}\\n\"\n",
    "        f\"P90 = {p90:.3f}\"\n",
    "    )\n",
    "    ax.text(0.97, 0.97, txt, transform=ax.transAxes,\n",
    "            ha=\"right\", va=\"top\")\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Anomalía (°C)\")\n",
    "    ax.set_ylabel(\"Densidad\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # leyenda mínima (para entender qué es cada línea)\n",
    "    # Nota: para no duplicar labels, hacemos handles manuales simples\n",
    "    from matplotlib.lines import Line2D\n",
    "    handles = [\n",
    "        Line2D([0],[0], color='k', lw=2, linestyle='-',  label='μ'),\n",
    "        Line2D([0],[0], color='k', lw=2, linestyle='--', label='μ±σ (Z=±1)'),\n",
    "        Line2D([0],[0], color='k', lw=2, linestyle=':',  label='P84 / P90'),\n",
    "    ]\n",
    "    ax.legend(handles=handles, loc=\"upper left\", frameon=True)\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 4), sharey=True)\n",
    "\n",
    "plot_hist_gauss_with_z(\n",
    "    axs[0],\n",
    "    Anoms_ATL3_JJ_a,\n",
    "    title=\"ATL3 (JJ) – Histograma + Normal + Z y percentiles\",\n",
    "    bins=10\n",
    ")\n",
    "\n",
    "plot_hist_gauss_with_z(\n",
    "    axs[1],\n",
    "    Anoms_Niño3_ND_a,\n",
    "    title=\"Niño (ND) – Histograma + Normal + Z y percentiles\",\n",
    "    bins=10\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9b3360-9ca7-42c2-9a34-fbd1627bf4b1",
   "metadata": {},
   "source": [
    "![Histograma ATL3 y Niño3.4](img/9.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92b23a9-85ee-4132-be1b-ce968cc65530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Detection_Plot_ENSO_ONI_with_intensity(year1, year2):\n",
    "    criterio = 0.5\n",
    "    ntri = 5\n",
    "    months_sel = [10, 11]  # Nov-Dic\n",
    "    \n",
    "    # Datos globales\n",
    "    var_a = np.asarray(variacion_mensual_Niño_a, dtype=float)\n",
    "    var_b = np.asarray(variacion_mensual_Niño_b, dtype=float)\n",
    "\n",
    "    # 1. Series temporales (Usando NIÑO_lats / NIÑO_lons)\n",
    "    niño1_a, niño1_b = niño_monthly_series(year1, NIÑO_lats[0], NIÑO_lats[1], NIÑO_lons[0], NIÑO_lons[1])\n",
    "    niño2_a, niño2_b = niño_monthly_series(year2, NIÑO_lats[0], NIÑO_lats[1], NIÑO_lons[0], NIÑO_lons[1])\n",
    "    \n",
    "    serie_a = np.r_[niño1_a, niño2_a]\n",
    "    serie_b = np.r_[niño1_b, niño2_b]\n",
    "\n",
    "    # 2. Detección ONI\n",
    "    tri_a, tri_lab_year, centers_idx = three_month_running(serie_a, year1, year2)\n",
    "    tri_b, _,            _           = three_month_running(serie_b, year1, year2)\n",
    "\n",
    "    valid_tri_a = runs(tri_a >= criterio, ntri)\n",
    "    valid_tri_b = runs(tri_b >= criterio, ntri)\n",
    "\n",
    "    valid_month_a = mark_center_months(valid_tri_a, centers_idx)\n",
    "    valid_month_b = mark_center_months(valid_tri_b, centers_idx)\n",
    "    valid1_a, valid2_a = valid_month_a[:12], valid_month_a[12:]\n",
    "    valid1_b, valid2_b = valid_month_b[:12], valid_month_b[12:]\n",
    "\n",
    "    # 3. Estadísticas\n",
    "    mean_a, sig_a, z_a = calculate_seasonal_stats(niño1_a, var_a, months_sel)\n",
    "    mean_b, sig_b, z_b = calculate_seasonal_stats(niño1_b, var_b, months_sel)\n",
    "    \n",
    "    cls_a = classify_by_z(z_a, Z_THR_WEAK, Z_THR_MOD, Z_THR_STRONG)\n",
    "    cls_b = classify_by_z(z_b, Z_THR_WEAK, Z_THR_MOD, Z_THR_STRONG)\n",
    "\n",
    "    # 4. Plots\n",
    "    ymin = np.nanmin([np.nanmin(serie_a), np.nanmin(serie_b)])\n",
    "    ymax = np.nanmax([np.nanmax(serie_a), np.nanmax(serie_b)])\n",
    "    pad = 0.05 * (ymax - ymin) if (ymax > ymin) else 0.1\n",
    "    ylim = (ymin - pad, ymax + pad)\n",
    "\n",
    "    fig = plt.figure(figsize=(14, 10))\n",
    "    gs = fig.add_gridspec(2, 2, height_ratios=[1, 1])\n",
    "    ax00 = fig.add_subplot(gs[0, 0])\n",
    "    ax01 = fig.add_subplot(gs[0, 1])\n",
    "    ax10 = fig.add_subplot(gs[1, :])\n",
    "\n",
    "    def plot_year_panel(ax, data_a, data_b, val_a, val_b, title):\n",
    "        ax.plot(meses, data_a, '-ro', markersize=2, label='HadISST')\n",
    "        ax.plot(meses, data_b, '-bo', markersize=2, label='ERSST')\n",
    "        ax.set_title(title)\n",
    "        ax.set_ylim(ylim)\n",
    "        ax.grid(True)\n",
    "        ax.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.3f'))\n",
    "        if np.any(val_a): ax.scatter(np.array(meses)[val_a], np.array(data_a)[val_a], s=18, marker='s', color='red', zorder=3)\n",
    "        if np.any(val_b): ax.scatter(np.array(meses)[val_b], np.array(data_b)[val_b], s=18, marker='s', color='blue', zorder=3)\n",
    "        ax.scatter(np.array(meses)[months_sel], np.array(data_a)[months_sel], s=60, marker='D', color='red', zorder=4)\n",
    "        ax.scatter(np.array(meses)[months_sel], np.array(data_b)[months_sel], s=60, marker='D', color='blue', zorder=4)\n",
    "        ax.legend()\n",
    "\n",
    "    plot_year_panel(ax00, niño1_a, niño1_b, valid1_a, valid1_b, f'Niño3.4 anomalies ({year1})')\n",
    "    plot_year_panel(ax01, niño2_a, niño2_b, valid2_a, valid2_b, f'Niño3.4 anomalies ({year2})')\n",
    "\n",
    "    # ONI Plot\n",
    "    x = np.arange(len(tri_a))\n",
    "    ax10.plot(x, tri_a, '-ro', markersize=3, label='HadISST (3-mo)')\n",
    "    ax10.plot(x, tri_b, '-bo', markersize=3, label='ERSST (3-mo)')\n",
    "    ax10.axhline(criterio, linestyle='--', linewidth=1.5)\n",
    "    ax10.grid(True)\n",
    "    ax10.set_title('ONI-like detection')\n",
    "    ax10.set_xticks(x)\n",
    "    ax10.set_xticklabels(tri_lab_year)\n",
    "    if np.any(valid_tri_a): ax10.scatter(x[valid_tri_a], tri_a[valid_tri_a], s=35, marker='s', color='red', zorder=3)\n",
    "    if np.any(valid_tri_b): ax10.scatter(x[valid_tri_b], tri_b[valid_tri_b], s=35, marker='s', color='blue', zorder=3)\n",
    "    ax10.legend()\n",
    "\n",
    "    txt_det_a = 'HadISST: Yes' if np.any(valid_tri_a) else 'HadISST: No'\n",
    "    txt_det_b = 'ERSST: Yes'  if np.any(valid_tri_b) else 'ERSST: No'\n",
    "    \n",
    "    fig.text(\n",
    "        0.5, 0.04,\n",
    "        f\"+{criterio:.1f} °C   5 consecutive overlapping 3-month periods -> {txt_det_a} | {txt_det_b}\",\n",
    "        ha='center', va='bottom', fontsize=10\n",
    "    )\n",
    "    fig.text(0.5, 0.01,\n",
    "        f\"HadISST ({year1}-{year2}): mean_season={mean_a:.2f}°C, σ_season={sig_a:.2f}, Z={z_a:.2f}, class={cls_a}   ||   \"\n",
    "        f\"ERSST ({year1}-{year2}): mean_season={mean_b:.2f}°C, σ_season={sig_b:.2f}, Z={z_b:.2f}, class={cls_b}\",\n",
    "        ha=\"center\", va=\"bottom\", fontsize=10\n",
    "    )\n",
    "    plt.tight_layout(rect=[0, 0.06, 1, 1])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac4fa66-35c9-4140-a346-330f292d57e2",
   "metadata": {},
   "source": [
    "#POSSIBLE ENSO ENTRE 1972-1973\n",
    "\n",
    "![Detection_Plot_ENSO_ONI_with_intensity(1972, 1973)](img/10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991e099c-f765-4434-9936-1442700faa90",
   "metadata": {},
   "source": [
    "#POSSIBLE ENSO ENTRE 1982-1983\n",
    "\n",
    "![Detection_Plot_ENSO_ONI_with_intensity(1982, 1983)](img/11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46442aa4-9372-439d-a3ae-298798979991",
   "metadata": {},
   "source": [
    "#POSSIBLE ENSO ENTRE 1987-1988\n",
    "\n",
    "![Detection_Plot_ENSO_ONI_with_intensity(1987, 1988)](img/12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747eb5e6-db38-440a-b80c-d8358522b87a",
   "metadata": {},
   "source": [
    "#Detection_Plot_ENSO_ONI_with_intensity(1986, 1987) \n",
    "\n",
    "![Detection_Plot_ENSO_ONI_with_intensity(1987, 1988)](img/13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac4b395-f7c6-4e9f-a5f4-2b3cec230fdc",
   "metadata": {},
   "source": [
    "#POSSIBLE ENSO ENTRE 1997-1998\n",
    "\n",
    "![Detection_Plot_ENSO_ONI_with_intensity(1997, 1998)](img/14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b09845-8182-4d60-8f3e-844ff9a7fad4",
   "metadata": {},
   "source": [
    "#POSSIBLE ENSO ENTRE 2015-2016\n",
    "\n",
    "![Detection_Plot_ENSO_ONI_with_intensity(2015, 2016)](img/15.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c30667-1a73-4659-9473-f25056528de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Detection_Plot_ATL3(year0):\n",
    "    months_sel = [5, 6] # Jun-Jul\n",
    "    criterio = 0.4\n",
    "    nmeses = 3\n",
    "    \n",
    "    # 1. Obtener Datos (Usando ATL3_lats / ATL3_lons)\n",
    "    niño_atl_a, niño_atl_b = niño_monthly_series(year0, ATL3_lats[0], ATL3_lats[1], ATL3_lons[0], ATL3_lons[1])\n",
    "    var_a = np.asarray(variacion_mensual_ATL3_a, dtype=float)\n",
    "    var_b = np.asarray(variacion_mensual_ATL3_b, dtype=float)\n",
    "\n",
    "    # 2. Detección\n",
    "    valid_a = runs(np.asarray(niño_atl_a) >= criterio, nmeses)\n",
    "    valid_b = runs(np.asarray(niño_atl_b) >= criterio, nmeses)\n",
    "\n",
    "    # 3. Estadísticas\n",
    "    mean_a,sig_season_a, z_a = calculate_seasonal_stats(niño_atl_a, var_a, months_sel)\n",
    "    mean_b, sig_season_b, z_b = calculate_seasonal_stats(niño_atl_b, var_b, months_sel)\n",
    "\n",
    "    cls_a = classify_by_z(z_a, Z_THR_WEAK, Z_THR_MOD, Z_THR_STRONG)\n",
    "    cls_b = classify_by_z(z_b, Z_THR_WEAK, Z_THR_MOD, Z_THR_STRONG)\n",
    "\n",
    "\n",
    "    # 4. Gráficos\n",
    "    ymin = np.nanmin([np.nanmin(niño_atl_a), np.nanmin(niño_atl_b)])\n",
    "    ymax = np.nanmax([np.nanmax(niño_atl_a), np.nanmax(niño_atl_b)])\n",
    "    pad = 0.05 * (ymax - ymin) if (ymax > ymin) else 0.1\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    ax[0].plot(meses, niño_atl_a, '-ro', markersize=2, label='HadISST')\n",
    "    ax[0].plot(meses, niño_atl_b, '-bo', markersize=2, label='ERSST')\n",
    "    ax[0].set_ylim(ymin - pad, ymax + pad)\n",
    "    ax[0].grid()\n",
    "    ax[0].set_title(f'ATL3 anomalies ({year0})')\n",
    "    \n",
    "    if np.any(valid_a): ax[0].scatter(np.array(meses)[valid_a], np.array(niño_atl_a)[valid_a], s=18, marker='s', color='red')\n",
    "    if np.any(valid_b): ax[0].scatter(np.array(meses)[valid_b], np.array(niño_atl_b)[valid_b], s=18, marker='s', color='blue')\n",
    "    ax[0].scatter(np.array(meses)[months_sel], np.array(niño_atl_a)[months_sel], s=60, marker='D', color='red')\n",
    "    ax[0].scatter(np.array(meses)[months_sel], np.array(niño_atl_b)[months_sel], s=60, marker='D', color='blue')\n",
    "    ax[0].legend()\n",
    "\n",
    "    ax[1].plot(meses, var_a, '-ro', markersize=2, label='HadISST')\n",
    "    ax[1].plot(meses, var_b, '-bo', markersize=2, label='ERSST')\n",
    "    ax[1].set_title('Variability (ATL3)')\n",
    "    ax[1].grid()\n",
    "    txt_det_a = 'HadISST: Yes' if np.any(valid_a) else 'HadISST: No'\n",
    "    txt_det_b = 'ERSST: Yes'  if np.any(valid_b) else 'ERSST: No'\n",
    "    fig.text(\n",
    "        0.5, 0.07,\n",
    "        f\"+{criterio:.1f} °C  {nmeses} consecutive months -> {txt_det_a} | {txt_det_b}\",\n",
    "        ha='center', va='bottom', fontsize=10\n",
    "    )\n",
    "\n",
    "    fig.text(\n",
    "        0.5, 0.03,\n",
    "        f\"HadISST({year0})  mean_season={mean_a:.2f}°C, σ_season={sig_season_a:.2f}, Z={z_a:.2f}, class={cls_a}   ||   \"\n",
    "        f\"ERSST({year0}) mean_season={mean_b:.2f}°C, σ_season={sig_season_b:.2f}, Z={z_b:.2f}, class={cls_b}\",\n",
    "        ha='center', va='bottom', fontsize=10\n",
    "    )\n",
    "    plt.tight_layout(rect=[0, 0.12, 1, 1])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe94376-06be-4161-b699-0d53da4de109",
   "metadata": {},
   "source": [
    "#POSSIBLE Atlantic Niño en 2010\n",
    "\n",
    "![Detection_Plot_ATL3(2010)](img/16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ca66bf-14a0-4861-8eb8-7f0d3738a231",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#POSSIBLE Atlantic Niño en 2016\n",
    "\n",
    "![Detection_Plot_ATL3(2016)](img/17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009d38c1-65d1-4a01-8bbe-80258d48c1ad",
   "metadata": {},
   "source": [
    "#POSSIBLE Atlantic Niño en 2018\n",
    "\n",
    "![Detection_Plot_ATL3(2018)](img/18.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b115e813-0fd0-4f57-b3b8-da700a17fcec",
   "metadata": {},
   "source": [
    "#POSSIBLE Atlantic Niño en 2019\n",
    "\n",
    "![Detection_Plot_ATL3(2018)](img/19.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7132ba-5744-47e2-8867-86c94fe51ae6",
   "metadata": {},
   "source": [
    "#POSSIBLE Atlantic Niño en 2020\n",
    "\n",
    "![Detection_Plot_ATL3(2029)](img/20.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab329101-9cea-4886-920a-62a8a4faee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NUEVOS TOP 5 PARA ATL3 y Niño3.4, vemos que buscar los años donde en esa zona las anomalias sean mas grandes no sirve de forma anual para ATL3\n",
    "#Vamos a fijarnos solo en los meses junio julio y agosto de cada año del Altantic Niño y para Septiembre Octubre Noviembre diciembre del Niño pacific.\n",
    "#ATL3\n",
    "\n",
    "niño = top5(years, Anoms_Niño3_ND_a,Anoms_Niño3_ND_b, N) #recordamos years=(np.arange(1960, 2021, 1))\n",
    "atl3 = top5(years, Anoms_ATL3_JJ_a,Anoms_ATL3_JJ_b, N) #recordamos years=(np.arange(1960, 2021, 1))\n",
    "\n",
    "\n",
    "\n",
    "fmt = {\"HadISST (°C)\": \"{:.2f}\", \"ERSST (°C)\": \"{:.2f}\"}\n",
    "header_style = [{\"selector\":\"th\",\"props\":[(\"padding-top\",\"2px\"),(\"padding-bottom\",\"2px\"),\n",
    "                                         (\"vertical-align\",\"bottom\"),(\"text-align\",\"center\")]}]\n",
    "\n",
    "s_atl3 = atl3.style.format(fmt).set_properties(**{\"text-align\":\"center\"}).set_table_styles(header_style)\n",
    "s_niño = niño.style.format(fmt).set_properties(**{\"text-align\":\"center\"}).set_table_styles(header_style)\n",
    "\n",
    "html = f\"\"\"\n",
    "<div style=\"text-align:left; font-weight:bold; font-size:15px; margin-bottom:12px;\">\n",
    "Picos de Anomalias (1960–2020)\n",
    "</div>\n",
    "\n",
    "<div style=\"display:flex; justify-content:left; gap:80px;\">\n",
    "  <div>\n",
    "    <div style=\"font-weight:bold; margin-bottom:6px;\">Atlantic Niño (ATL3) para Junio/Julio:</div>\n",
    "    {s_atl3.to_html()}\n",
    "  </div>\n",
    "  <div>\n",
    "    <div style=\"font-weight:bold; margin-bottom:6px;\">Pacific Niño (Niño 3.4) Noviembre/Diciembre:</div>\n",
    "    {s_niño.to_html()}\n",
    "  </div>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(html))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815a7fcf-57e0-416f-bba4-ef305c675709",
   "metadata": {},
   "source": [
    "#POSSIBLE ENSO ENTRE 1965-1966\n",
    "\n",
    "![Detection_Plot_ENSO_ONI_with_intensity(1965, 1966)](img/21.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d086fe-3da1-4aac-aee4-c7a1ebfcc67d",
   "metadata": {},
   "source": [
    "#POSSIBLE Atlantic Niño en 1963\n",
    "\n",
    "![Detection_Plot_ATL3(1963)](img/22.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6985651c-7bdf-4ade-9a04-1493058fb809",
   "metadata": {},
   "source": [
    "#POSSIBLE Atlantic Niño en 1966\n",
    "\n",
    "![Detection_Plot_ATL3(1966)](img/23.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d7539f-a233-4c1b-9408-cd57d877b55e",
   "metadata": {},
   "source": [
    "#POSSIBLE Atlantic Niño en 1968\n",
    "\n",
    "![Detection_Plot_ATL3(1968)](img/24.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c119917d-94bc-42c1-801e-a57d299a63ee",
   "metadata": {},
   "source": [
    "#POSSIBLE Atlantic Niño en 1987\n",
    "\n",
    "![Detection_Plot_ATL3(1987)](img/25.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5813484a-0f22-439d-a27e-c040e0cfe5c3",
   "metadata": {},
   "source": [
    "#POSSIBLE Atlantic Niño en 1988\n",
    "\n",
    "![Detection_Plot_ATL3(1988)](img/26.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d3a161-1e16-46c4-aa2e-11002b90da24",
   "metadata": {},
   "source": [
    "#POSSIBLE Atlantic Niño en 1996\n",
    "\n",
    "![Detection_Plot_ATL3(1996)](img/27.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf29a078-cc10-4ba3-965e-6bc9a9e056bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot_global_mean_anoms_with_Z(dataset, year1, year2, year3, year4, months_sel, min_abs_temp=3.0, event=None):\n",
    "    months_sel = list(months_sel)\n",
    "    if not months_sel: raise ValueError(\"Selecciona al menos 1 mes.\")\n",
    "    season_lbl = f\"{meses[months_sel[0]]}\" if len(months_sel)==1 else f\"{meses[months_sel[0]]}–{meses[months_sel[-1]]}\"\n",
    "    \n",
    "    ds = str(dataset).lower()\n",
    "    if ds == \"a\":\n",
    "        anoms, lons, lats = anoms_a, lon_a, lat_a\n",
    "        sigma_enso = np.asarray(variacion_mensual_Niño_a, dtype=float)\n",
    "        sigma_atl3 = np.asarray(variacion_mensual_ATL3_a, dtype=float)\n",
    "        pick, ds_lbl = 0, \"HadISST\"\n",
    "    elif ds == \"b\":\n",
    "        anoms, lons, lats = anoms_b, lon_b, lat_b\n",
    "        sigma_enso = np.asarray(variacion_mensual_Niño_b, dtype=float)\n",
    "        sigma_atl3 = np.asarray(variacion_mensual_ATL3_b, dtype=float)\n",
    "        pick, ds_lbl = 1, \"ERSST\"\n",
    "    else:\n",
    "        raise ValueError(\"dataset debe ser 'a' o 'b'.\")\n",
    "\n",
    "    # Helper Geográfico\n",
    "    def fix_lon_field(ln, fld):\n",
    "        ln = np.asarray(ln)\n",
    "        if np.nanmax(ln) > 180: ln = ((ln + 180) % 360) - 180\n",
    "        idx = np.argsort(ln)\n",
    "        return ln[idx], fld[:, idx]\n",
    "    \n",
    "    def centers_to_edges(c):\n",
    "        mid = 0.5 * (c[:-1] + c[1:])\n",
    "        return np.concatenate([[c[0] - (mid[0]-c[0])], mid, [c[-1] + (c[-1]-mid[-1])]])\n",
    "\n",
    "    # --- MODO GIF ---\n",
    "    if event is not None:\n",
    "        years_anim = [year1] if (year2 == 0 or year2 is None) else [year1, year2]\n",
    "        frames = []\n",
    "        for y in years_anim:\n",
    "            a_y = anom_year(y, anoms, year0=1960)\n",
    "            for m in range(12): frames.append((y, m, a_y[m, :, :]))\n",
    "\n",
    "        vmax = float(min_abs_temp)\n",
    "        fig = plt.figure(figsize=(13, 6.5))\n",
    "        ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "        ax.add_feature(cfeature.COASTLINE, linewidth=0.7)\n",
    "        ax.add_feature(cfeature.LAND, facecolor=\"lightgray\")\n",
    "        \n",
    "        y0, m0, f0 = frames[0]\n",
    "        lons_fix, f0_fix = fix_lon_field(lons, f0)\n",
    "        lats_fix = lats[::-1] if (len(lats)>1 and lats[1]<lats[0]) else lats\n",
    "        if len(lats)>1 and lats[1]<lats[0]: f0_fix = f0_fix[::-1, :]\n",
    "\n",
    "        mesh = ax.pcolormesh(centers_to_edges(lons_fix), centers_to_edges(lats_fix), np.ma.masked_invalid(f0_fix),\n",
    "                             transform=ccrs.PlateCarree(), shading=\"auto\", vmin=-vmax, vmax=vmax, cmap=plt.cm.RdBu_r)\n",
    "        \n",
    "        plt.colorbar(mesh, ax=ax, orientation=\"horizontal\", pad=0.06).set_label(\"SST anomaly (°C)\")\n",
    "        title = ax.set_title(f\"{ds_lbl} | {event} | {y0} {meses[m0]}\")\n",
    "        \n",
    "        def update(i):\n",
    "            y, m, f = frames[i]\n",
    "            _, f_fix = fix_lon_field(lons, f)\n",
    "            if len(lats)>1 and lats[1]<lats[0]: f_fix = f_fix[::-1, :]\n",
    "            mesh.set_array(np.ma.masked_invalid(f_fix).ravel())\n",
    "            title.set_text(f\"Global SST anomalies | {event} | {y} {meses[m]} | {ds_lbl}\")\n",
    "            return [mesh, title]\n",
    "            \n",
    "        ani = animation.FuncAnimation(fig, update, frames=len(frames), interval=500)\n",
    "        out_gif = (f\"global_SST_anoms_{ds_lbl}_{event.replace(' ', '_')}_{years_anim[0]}\"+ (f\"_{years_anim[1]}\" if len(years_anim) == 2 else \"\") + \".gif\")\n",
    "        ani.save(out_gif, writer=animation.PillowWriter(fps=2))\n",
    "        plt.close(fig)\n",
    "        display(Image(filename=out_gif))\n",
    "        return out_gif\n",
    "\n",
    "    # --- MODO 2x2 ---\n",
    "    years = [year1, year2, year3, year4]\n",
    "    fields, Z_enso, Z_atl3 = [], [], []\n",
    "\n",
    "    for y in years:\n",
    "        a_y = anom_year(y, anoms, year0=1960)\n",
    "        fields.append(np.mean(a_y[months_sel, :, :], axis=0))\n",
    "        \n",
    "        # Uso de NIÑO_lats/lons y ATL3_lats/lons\n",
    "        e_a, e_b = niño_monthly_series(y, NIÑO_lats[0], NIÑO_lats[1], NIÑO_lons[0], NIÑO_lons[1])\n",
    "        _, _, zE = calculate_seasonal_stats((e_a if pick==0 else e_b), sigma_enso, months_sel)\n",
    "        Z_enso.append(zE)\n",
    "\n",
    "        a_a, a_b = niño_monthly_series(y, ATL3_lats[0], ATL3_lats[1], ATL3_lons[0], ATL3_lons[1])\n",
    "        _, _, zA = calculate_seasonal_stats((a_a if pick==0 else a_b), sigma_atl3, months_sel)\n",
    "        Z_atl3.append(zA)\n",
    "\n",
    "    vmax = float(min_abs_temp)\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12), subplot_kw={\"projection\": ccrs.PlateCarree()})\n",
    "    \n",
    "    for ax, fld, y, zE, zA in zip(axes.ravel(), fields, years, Z_enso, Z_atl3):\n",
    "        ax.add_feature(cfeature.COASTLINE)\n",
    "        ax.add_feature(cfeature.LAND, facecolor=\"lightgray\")\n",
    "        fill = ax.contourf(lons, lats, fld, levels=np.linspace(-vmax, vmax, 13),\n",
    "                           cmap=plt.cm.RdBu_r, transform=ccrs.PlateCarree(), extend=\"both\")\n",
    "        ax.set_title(f\"{y} | Z_ENSO={zE:.2f}  Z_ATL3={zA:.2f}\")\n",
    "        plt.colorbar(fill, ax=ax, orientation=\"horizontal\", pad=0.05).set_label(\"SST anomaly (°C)\")\n",
    "\n",
    "    fig.suptitle(f\"Global SST anomalies ({season_lbl}) — {ds_lbl}\", fontsize=14, y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6212dc3f-aa21-41e6-93c9-dba82055ab96",
   "metadata": {},
   "source": [
    "#FIGURA YEAR-CRITERIA  \n",
    "\n",
    "![Plot_global_mean_anoms_with_Z(\"a\", 1963, 1968, 1987, 1996, months_sel=[5,6], min_abs_temp=1.5, event=None) ](img/28.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6371e498-919a-42d6-9f1c-c8078e28f16c",
   "metadata": {},
   "source": [
    "#Altanic Niño in 1960-1996-1968-1963\n",
    "\n",
    "![Plot_global_mean_anoms_with_Z(\"b\", 1960,1996 ,1968 ,1963 , [5,6], min_abs_temp=1.5)](img/29.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41db47a6-278b-4e8c-b3eb-0c6a5f02767b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot_global_mean_anoms_with_Z(\"a\", 1963, 0, 1972, 2015, months_sel=[10,11], min_abs_temp=2, event=\"Atlantic Niño\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615f9b22-d4ff-49bf-8c3e-6d46bbecdf3a",
   "metadata": {},
   "source": [
    "#Pacific Niño in 1960-1987-1972-2015\n",
    "\n",
    "\n",
    "![Plot_global_mean_anoms_with_Z(\"a\", 1960, 1987, 1972, 2015, months_sel=[10,11], min_abs_temp=3)](img/30.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff50a6bf-d23b-4a12-b6ab-a457589b8146",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot_global_mean_anoms_with_Z(\"a\", 2015, 2016, 1972, 2015, months_sel=[10,11], min_abs_temp=3, event=\"Pacific Niño\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55a9c46-20ca-4151-8836-57c745cfaa50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
