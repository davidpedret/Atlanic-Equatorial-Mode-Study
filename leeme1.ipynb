{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76d7efbc-d0ad-4755-8ef4-336f5177f640",
   "metadata": {},
   "source": [
    "# Atlantic-Eq Mode Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01527630-b4f9-466f-9611-8a3fb0ee47c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Librerías Estándar \n",
    "import random\n",
    "import warnings\n",
    "\n",
    "# Análisis de Datos y Matemáticas \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from scipy import stats\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "from netCDF4 import Dataset as ncread  # Asumiendo que usas ncread explícitamente\n",
    "\n",
    "# Visuals\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib import animation\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "#Utilidades y Entorno (Jupyter/IPython) ---\n",
    "from tabulate import tabulate\n",
    "from IPython.display import display, HTML, Image  # Todo en una sola línea\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2484670-0c68-4476-9ece-1154f9d9062a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'E:/TFG/Datos/Importants/'\n",
    "data1 = path+'HadISST_sst_v1.1_196001_202105.nc'#database_a: HadISST (Hadley Centre Sea Ice and Sea Surface Temperature dataset) de 1960-2021, 1°x1°\n",
    "data2 = path+'sst.mnmean_v5_196001_202105.nc'   #database_b: ERSST (Extended Reconstructed SST), 2°x2° from NOAA\n",
    "data3 = path+'noaa.pcp.mon.anom_196001_202105.nc' #datobase1: NOAA (National Oceanic and Atmospheric Administration ), 1.875°x2° \n",
    "data4 = path+'pcp.mon.ncep-ncar_196001_202105.nc' #datobase2: NCEP-NCAR, 2.5°x2.5°\n",
    "#Retrieve data\n",
    "nc1 = ncread(data1, 'r')  #sst\n",
    "nc2 = ncread(data2, 'r')  #sst\n",
    "nc3 = ncread(data3, 'r')  #prec\n",
    "nc4 = ncread(data4, 'r')  #prec\n",
    "#vamos a ver las variables que tenemos que llamar de cada archivo\n",
    "var1 = nc1.variables[\"sst\"]\n",
    "var2 = nc2.variables[\"sst\"]\n",
    "var3 = nc3.variables[\"precip\"]\n",
    "var4 = nc4.variables[\"prate\"]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def summarize_var(var, nt=None, p=(1, 99)):\n",
    "    x = var[:] if nt is None else var[:nt]\n",
    "    if isinstance(x, np.ma.MaskedArray):\n",
    "        masked = int(np.sum(x.mask))\n",
    "        x = np.ma.filled(x, np.nan)\n",
    "    else:\n",
    "        masked = 0\n",
    "    x = x.astype(float)\n",
    "    vals = x[np.isfinite(x)]\n",
    "    \n",
    "    return {\"units\": getattr(var, \"units\", None),\n",
    "        \"p1\": np.percentile(vals, p[0]) if vals.size else np.nan,\n",
    "        \"p99\": np.percentile(vals, p[1]) if vals.size else np.nan,\n",
    "        \"min\": np.min(vals) if vals.size else np.nan,\n",
    "        \"max\": np.max(vals) if vals.size else np.nan,\n",
    "        \"n_total\": x.size,\n",
    "        \"masked_count\": masked,\n",
    "        \"frac_finite\": vals.size / x.size if x.size else np.nan}\n",
    "    \n",
    "s1 = summarize_var(var1, nt=732)\n",
    "s2 = summarize_var(var2, nt=732)\n",
    "s3 = summarize_var(var3, nt=732)\n",
    "s4 = summarize_var(var4, nt=732)\n",
    "# Vamos a crear una tabla para mostrar la naturaleza de los datos:\n",
    "\n",
    "vars_dict = {\"HadISST SST\": var1,\"ERSST SST\": var2,\"NOAA PCP\": var3,\"NCEP-NCAR PCP\": var4}\n",
    "table = []\n",
    "for name, var in vars_dict.items():\n",
    "    s = summarize_var(var, nt=732)\n",
    "    table.append([name,s[\"units\"],(s[\"p1\"], s[\"p99\"]),(s[\"min\"], s[\"max\"]),s[\"frac_finite\"]])\n",
    "    columns = [\"Base de datos\",\"Unidades\",\"p1 / p99\",\"Min / Max\",\"Fracción válida\"]\n",
    "\n",
    "df_table = pd.DataFrame(table, columns=columns)\n",
    "df_table = df_table.set_index(\"Base de datos\")\n",
    "\n",
    "df_table[\"p1 / p99\"] = df_table[\"p1 / p99\"].apply(lambda x: f\"({x[0]:.2f}, {x[1]:.2f})\")\n",
    "\n",
    "df_table[\"Min / Max\"] = df_table[\"Min / Max\"].apply(lambda x: f\"({x[0]:.2f}, {x[1]:.2f})\")\n",
    "\n",
    "df_table[\"Fracción válida\"] = df_table[\"Fracción válida\"].apply(lambda x: f\"{x:.2f}\")\n",
    "print(tabulate(df_table, headers=\"keys\", tablefmt=\"pretty\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080e573b-23e7-41e6-98cc-9b4c34e7488b",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "background-color:#f7f7f7;\n",
    "border-left:5px solid #444;\n",
    "padding:14px;\n",
    "border-radius:4px;\n",
    "\">\n",
    "Las keys indican que palabra clave esta asociada a cada dato según cada base de datos, vemos que son diferentes en el primer caso por lo que crearemos una funcion que sepa leerlo bien.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6273c7-94da-42cc-801e-98eac5867d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\"HadISST\": data1,\"ERSST\": data2,\"NOAA PCP\": data3,\"NCEP-NCAR\": data4}\n",
    "\n",
    "# Función para extraer info de los datos, resolución, rango, etc\n",
    "def ex_info(file_path):\n",
    "    \n",
    "    nc = ncread(file_path, \"r\")  # Abrir archivo NetCDF\n",
    "\n",
    "     # Verificamos cómo están nombradas las variables de latitud y longitud\n",
    "    lat_name = next(var for var in nc.variables.keys() if \"lat\" in var.lower())  #next devuele el primer elemento del iterador \n",
    "    lon_name = next(var for var in nc.variables.keys() if \"lon\" in var.lower())  #util porque no queremos una lista de 1 elemento queremos el string\n",
    "                                                                                     #lat_name[0] tambien devuelve el primero pero el next te evita\n",
    "    latitudes = nc.variables[lat_name][:]                                        #crear la lista, es mas eficiente\n",
    "    longitudes = nc.variables[lon_name][:]\n",
    "\n",
    "    # Rango de latitud y longitud\n",
    "    lat_range = (latitudes.min(), latitudes.max())\n",
    "    lon_range = (longitudes.min(), longitudes.max())\n",
    "\n",
    "    # Resolución (suponemos que la diferencia entre dos puntos consecutivos es la resolución)\n",
    "    lat_res = round(abs(latitudes[1] - latitudes[0]),3)\n",
    "    lon_res = round(abs(longitudes[1] - longitudes[0]),3)\n",
    "\n",
    "    nc.close()  # Cerrar el archivo para liberar memoria\n",
    "    return (lat_res, lon_res), lat_range, lon_range\n",
    "\n",
    "# Crear la tabla con los datos extraídos\n",
    "table = []\n",
    "for name, path in data_files.items(): #Este bucle recorre cada par (nombre, ruta) del diccionario.\n",
    "    info = ex_info(path)  #Pathh en este contexto seria el segundo numero de la dupla es decir data1,data2,etc\n",
    "    if info:\n",
    "        table.append([name, *info]) #con el * desempacamos la tupla y la tabla es [HadISST, (1,2),(3,4),(5,6)] y no [HadISST,[(1,2),(3,4),(5,6)]]\n",
    "        \n",
    "# Crear DataFrame\n",
    "columns = [\"Base de datos\", \"Resolución (lat, lon)\", \"Rango de latitud\", \"Rango de longitud\",]\n",
    "df_table = pd.DataFrame(table, columns=columns)\n",
    "df_table = df_table.set_index(\"Base de datos\")\n",
    "\n",
    "\n",
    "# Hacemos bonita la tabla\n",
    "df_table[\"Rango de latitud\"] = df_table[\"Rango de latitud\"].apply(lambda x: f\"({round(float(x[0]), 2)}°, {round(float(x[1]), 2)}°)\")\n",
    "df_table[\"Rango de longitud\"] = df_table[\"Rango de longitud\"].apply( lambda x: f\"({round(float(x[0]), 2)}°, {round(float(x[1]), 2)}°)\")\n",
    "df_table[\"Resolución (lat, lon)\"] = df_table[\"Resolución (lat, lon)\"].apply(lambda x: f\"({round(float(x[0]), 3)}°, {round(float(x[1]), 3)}°)\")\n",
    "\n",
    "print(tabulate(df_table, headers=\"keys\", tablefmt=\"pretty\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca693be-c150-4c7b-8381-eb037365bb14",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "background-color:#f7f7f7;\n",
    "border-left:5px solid #444;\n",
    "padding:14px;\n",
    "border-radius:4px;\n",
    "\">\n",
    "<b>Tabla de referencia para la selección de rangos</b><br><br>\n",
    "\n",
    "Esta tabla es necesaria para gestionar de forma coherente cada una de las bases de datos utilizadas.\n",
    "Actúa como una <b>guía metodológica</b> que permite seleccionar correctamente los distintos conjuntos\n",
    "de rangos espaciales y temporales empleados en el análisis.\n",
    "\n",
    "Dado que cada base de datos presenta diferencias en resolución espacial, cobertura y sistema de referencia,\n",
    "la utilización de esta tabla garantiza que los rangos escogidos sean consistentes y comparables entre productos.\n",
    "De este modo, se evitan errores derivados de una selección incorrecta de índices.\n",
    "\n",
    "Las dos bases de datos empleadas presentan rangos espaciales similares en latitud, pero utilizan\n",
    "convenciones distintas en longitud, lo que debe tenerse en cuenta para una correcta delimitación\n",
    "de las regiones de estudio.\n",
    "\n",
    "En latitud, los rangos son:\n",
    "<ul>\n",
    "<li><b>ERSST:</b> de −88° a 88°</li>\n",
    "<li><b>HadISST:</b> de −89.5° a 89.5°</li>\n",
    "</ul>\n",
    "\n",
    "En longitud, las diferencias son más relevantes:\n",
    "\n",
    "<b>HadISST</b> utiliza una convención centrada en el meridiano de Greenwich, con longitudes comprendidas\n",
    "entre −179.5° y 179.5°, ordenadas de oeste a este:\n",
    "\n",
    "<div style=\"font-family: monospace; font-size:13px; width:420px; margin:10px auto;\">\n",
    "<span style=\"display:block; text-align:center;\">−180°&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0°&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;180°</span>\n",
    "<span style=\"display:block; text-align:center;\">│────────────────────│────────────────────│</span>\n",
    "<span style=\"display:block; text-align:center;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Greenwich</span>\n",
    "</div\n",
    "\n",
    "<b>ERSST</b>, en cambio, emplea una convención de 0° a 358°, también ordenada de oeste a este, donde el\n",
    "meridiano de Greenwich corresponde a 0° y 360° representa de nuevo el mismo meridiano:\n",
    "\n",
    "<div style=\"font-family: monospace; font-size:13px; width:420px; margin:10px auto;\">\n",
    "<span style=\"display:block; text-align:right;\">0°&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;180°&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;360° (= 0°)</span>\n",
    "<span style=\"display:block; text-align:center;\">│────────────────────│─────────────────────│</span>\n",
    "<span style=\"display:block; text-align:right;\">Greenwich&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Océano&nbsp;Pacífico&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Greenwich</span>\n",
    "</div>\n",
    "\n",
    "En el caso de <b>HadISST</b>, la interpretación de la longitud resulta directa.\n",
    "Sin embargo, en <b>ERSST</b> es fundamental tener en cuenta que el valor 0° corresponde al meridiano\n",
    "de Greenwich. Tener en cuenta esto es importante para la definición de áreas geográficas\n",
    "y, por tanto, en los cálculos espaciales posteriores.\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "</div\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435ef7ad-4967-408e-87f1-cffa1ab244fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entendemos nuestros datos:\n",
    "sst_a = nc1.variables['sst'][:732] #HadISST: R=1°x1°,\n",
    "sst_b = nc2.variables['sst'][:732] #ERSSTD:  R=2°x2°   \n",
    "pcp = nc3.variables['precip'][:732] #NOAA PCP:  R=2.5°x2.5°\n",
    "pcp2 = nc4.variables['prate'][:732] #NCEP-NCAR:  R=1.889°x1.875°\n",
    "\n",
    "\n",
    "def lat_lon(file_path): #funcion para retornar long y lat en funcion de la data\n",
    "    nc = ncread(file_path, \"r\")\n",
    "    lat_name = next(var for var in nc.variables.keys() if \"lat\" in var.lower()) #busca el nombre asociado las coordenadas\n",
    "    lon_name = next(var for var in nc.variables.keys() if \"lon\" in var.lower()) #suponemos que contiene lat/lon\n",
    "    longitudes = nc.variables[lon_name][:]\n",
    "    latitudes = nc.variables[lat_name][:]                                        \n",
    "    nc.close()\n",
    "    return longitudes, latitudes\n",
    "    \n",
    "lon_a, lat_a  = lat_lon(data1)\n",
    "lon_b, lat_b  = lat_lon(data2)\n",
    "lon_pcp, lat_pcp  = lat_lon(data3)\n",
    "lon_pcp2, lat_pcp2  = lat_lon(data4)\n",
    "\n",
    "nt = len(sst_a[:,0,0])\n",
    "nyr = int(nt/12) \n",
    "ny_a = len(lat_a)\n",
    "ni_a = len(lon_a)   # al haber diferentes resoluciones el numeros de lat y longitud varia asi que mejor mirar todas\n",
    "ny_b = len(lat_b) \n",
    "ni_b = len(lon_b)\n",
    "ny_pcp = len(lat_pcp)\n",
    "ni_pcp = len(lon_pcp)\n",
    "ny_pcp2 = len(lat_pcp2)\n",
    "ni_pcp2 = len(lon_pcp2)\n",
    "\n",
    "sst_a = np.where(sst_a <= -1.79, np.nan, sst_a)  # SST oceánica no baja de -1.8°C\n",
    "sst_a = np.where(sst_a > 40, np.nan, sst_a)    # SST oceánica no sube de ~35°C\n",
    "sst_b = np.where(sst_b <= -1.79, np.nan, sst_b)  # SST oceánica no baja de -1.8°C\n",
    "sst_b = np.where(sst_b > 40, np.nan, sst_b)    # SST oceánica no sube de ~35°C\n",
    "\n",
    "# Calcular medias anuales (732 meses = 61 años)\n",
    "sst_a_annual = sst_a.reshape(61, 12, *sst_a.shape[1:]).mean(axis=1)\n",
    "sst_b_annual = sst_b.reshape(61, 12, *sst_b.shape[1:]).mean(axis=1)\n",
    "\n",
    "Temp_ATL3_a = []\n",
    "Temp_ATL3_b = []\n",
    "Temp_Niño_a = []\n",
    "Temp_Niño_b = []\n",
    "\n",
    "for i in range(61):\n",
    "    matriuATL3_a = sst_a_annual[i, 86:94, 160:180]      # HadISST\n",
    "    Temp_ATL3_a.append(np.mean(matriuATL3_a))\n",
    "\n",
    "    matriuATL3_b = sst_b_annual[i, 42:47, 170:180]      # ERSST\n",
    "    Temp_ATL3_b.append(np.mean(matriuATL3_b))\n",
    "\n",
    "    matriuNiño_a = sst_a_annual[i, 84:96, 10:60]      # HadISST\n",
    "    Temp_Niño_a.append(np.mean(matriuNiño_a))\n",
    "\n",
    "    matriuNiño_b = sst_b_annual[i, 41:47, 95:120]      # ERSST\n",
    "    Temp_Niño_b.append(np.mean(matriuNiño_b))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "Temp_ATL3_a = np.array(Temp_ATL3_a)\n",
    "Temp_ATL3_b = np.array(Temp_ATL3_b)\n",
    "Temp_Niño_a = np.array(Temp_Niño_a)\n",
    "Temp_Niño_b = np.array(Temp_Niño_b)\n",
    "\n",
    "# Tendencias lineales\n",
    "years=(np.arange(1960, 2021, 1))\n",
    "# ATL3\n",
    "coef_ATL3_a = np.polyfit(years, Temp_ATL3_a, 1)\n",
    "coef_ATL3_b = np.polyfit(years, Temp_ATL3_b, 1)\n",
    "\n",
    "trend_ATL3_a = np.polyval(coef_ATL3_a, years)\n",
    "trend_ATL3_b = np.polyval(coef_ATL3_b, years)\n",
    "\n",
    "slope_ATL3_a_dec = coef_ATL3_a[0] * 10\n",
    "slope_ATL3_b_dec = coef_ATL3_b[0] * 10\n",
    "\n",
    "# Niño 3.4\n",
    "coef_Niño_a = np.polyfit(years, Temp_Niño_a, 1)\n",
    "coef_Niño_b = np.polyfit(years, Temp_Niño_b, 1)\n",
    "\n",
    "trend_Niño_a = np.polyval(coef_Niño_a, years)\n",
    "trend_Niño_b = np.polyval(coef_Niño_b, years)\n",
    "\n",
    "slope_Niño_a_dec = coef_Niño_a[0] * 10\n",
    "slope_Niño_b_dec = coef_Niño_b[0] * 10\n",
    "\n",
    "# Figura con dos paneles\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 5), sharex=True)\n",
    "\n",
    "# ATL3\n",
    "axs[0].plot(years, Temp_ATL3_a, 'o-', color='C0', alpha=0.4, label='HadISST (datos)')\n",
    "axs[0].plot(years, Temp_ATL3_b, 'o-', color='C1', alpha=0.4, label='ERSST (datos)')\n",
    "axs[0].plot(years, trend_ATL3_a, color='C0', lw=2.5, label='HadISST – Tendencia')\n",
    "axs[0].plot(years, trend_ATL3_b, color='C1', lw=2.5, label='ERSST – Tendencia')\n",
    "\n",
    "axs[0].set_title('Atlantic Niño (ATL3)')\n",
    "axs[0].set_ylabel('SST Media Anual (°C)')\n",
    "axs[0].set_xlim(1960, 2020)\n",
    "axs[0].grid(True, alpha=0.3)\n",
    "\n",
    "axs[0].text(\n",
    "    0.03, 0.95,\n",
    "    f\"HadISST: {slope_ATL3_a_dec:.2f} °C/década\\n\"\n",
    "    f\"ERSST:  {slope_ATL3_b_dec:.2f} °C/década\",\n",
    "    transform=axs[0].transAxes,\n",
    "    fontsize=10,\n",
    "    verticalalignment='top'\n",
    ")\n",
    "\n",
    "# Niño 3.4\n",
    "axs[1].plot(years, Temp_Niño_a, 'o-', color='C0', alpha=0.4, label='HadISST (datos)')\n",
    "axs[1].plot(years, Temp_Niño_b, 'o-', color='C1', alpha=0.4, label='ERSST (datos)')\n",
    "axs[1].plot(years, trend_Niño_a, color='C0', lw=2.5, label='HadISST – Tendencia')\n",
    "axs[1].plot(years, trend_Niño_b, color='C1', lw=2.5, label='ERSST – Tendencia')\n",
    "\n",
    "axs[1].set_title('Pacific Niño (Niño 3.4)')\n",
    "axs[1].set_xlim(1960, 2020)\n",
    "axs[1].grid(True, alpha=0.3)\n",
    "\n",
    "axs[1].text(\n",
    "    0.03, 0.95,\n",
    "    f\"HadISST: {slope_Niño_a_dec:.2f} °C/década\\n\"\n",
    "    f\"ERSST:  {slope_Niño_b_dec:.2f} °C/década\",\n",
    "    transform=axs[1].transAxes,\n",
    "    fontsize=10,\n",
    "    verticalalignment='top'\n",
    ")\n",
    "# Ejes comunes\n",
    "for ax in axs:\n",
    "    ax.set_xlabel('Años (1960–2020)')\n",
    "    ax.set_xticks(np.arange(1960, 2021, 5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99a620c-9844-4f7a-b548-e5988d45a625",
   "metadata": {},
   "source": [
    "![SST_mean](img/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83fe98c-8be3-492a-9b85-a8b8b63c04eb",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "background-color:#f7f7f7;\n",
    "border-left:5px solid #444;\n",
    "padding:14px;\n",
    "border-radius:4px;\n",
    "\">\n",
    "\n",
    "<b>Temperatura media anual y metodología de cálculo</b><br><br>\n",
    "Se muestran dos gráficas correspondientes a la temperatura media anual en las regiones del \n",
    "<b>Atlantic Niño (ATL3)</b> y del <b>Pacific Niño (Niño 3.4)</b>.\n",
    "En ambas regiones se observa una <b>clara tendencia positiva</b> de la temperatura a lo largo del periodo de estudio.\n",
    "Dado que esta señal está asociada al calentamiento global, es necesario aplicar un \n",
    "<b>detrend</b> previo al cálculo de las anomalías climáticas.\n",
    "Además, puesto que la magnitud de la tendencia difiere entre regiones, este detrend debe aplicarse\n",
    "<b>punto a punto</b> en cada celda espacial.\n",
    "\n",
    "\n",
    "\n",
    "Las dos bases de datos analizadas,\n",
    "<span style=\"color:#ADD8E6;\"><b>HadISST</b></span> y\n",
    "<span style=\"color:#FFA500;\"><b>ERSST</b></span>,\n",
    "presentan resultados coherentes con lo esperado.\n",
    "ERSST muestra sistemáticamente valores ligeramente más elevados, aunque dentro de un margen razonable.\n",
    "Estas diferencias se explican por el distinto sistema de observación, la frecuencia temporal,\n",
    "la resolución espacial y las metodologías de reconstrucción empleadas en cada producto,\n",
    "lo que introduce un pequeño <i>offset</i> entre ambas bases de datos.\n",
    "\n",
    "\n",
    "Se descartan los valores de temperatura superiores a <b>35&nbsp;°C</b> para evitar la contaminación por puntos de tierra,\n",
    "así como los valores inferiores a <b>−1.79&nbsp;°C</b>.\n",
    "Este último umbral es especialmente relevante en ERSST, donde la región cubierta por hielo en el Ártico\n",
    "presenta valores cercanos a −1.8&nbsp;°C que distorsionan la media y los cálculos posteriores.\n",
    "\n",
    "Para el cálculo de la temperatura media en cada región se ha utilizado inicialmente\n",
    "la media directa de la matriz de datos.\n",
    "No obstante, esta aproximación no es estrictamente correcta, ya que el área representada por cada celda\n",
    "depende de la latitud.\n",
    "La forma físicamente correcta de calcular la media espacial es ponderar cada celda\n",
    "por el coseno de la latitud, de acuerdo con la expresión:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a58cff-d7d0-4acb-9b4d-391fca8bd01e",
   "metadata": {},
   "source": [
    "$$\n",
    "\\overline{T}\n",
    "=\n",
    "\\frac{\\sum_{\\varphi,\\lambda} T(\\varphi,\\lambda)\\,\\cos(\\varphi)}\n",
    "     {\\sum_{\\varphi,\\lambda} \\cos(\\varphi)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b70c4a-9441-4404-acd2-30c3b6d64b22",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "background-color:#f7f7f7;\n",
    "border-left:5px solid #444;\n",
    "padding:14px;\n",
    "border-radius:4px;\n",
    "\">\n",
    "Esta ponderación es necesaria porque la Tierra es una esfera y el área de las celdas disminuye con la latitud.\n",
    "Para latitudes bajas, como las consideradas en este estudio, el error cometido al no aplicar esta corrección\n",
    "es muy reducido (del orden del <b>0.5&nbsp;%</b>), por lo que la aproximación resulta aceptable.\n",
    "\n",
    "De forma análoga, en el cálculo inicial se han utilizado los índices directos de la malla\n",
    "en lugar de interpolar exactamente los límites geográficos de cada región.\n",
    "Esto puede introducir pequeñas discrepancias, pero dado que las áreas analizadas son amplias\n",
    "y se encuentran próximas al ecuador, el impacto debido al cambio de latitud (coger valores de latitud diferentes porque estan en nuestra malla) es mínimo. Además no estudiar las zonas exactas por decimas de grados no es un problema ya los gradientes meridionales cerca del ecuador son suaves.\n",
    "\n",
    "En la siguiente celda se repite el cálculo sin ninguna de estas aproximaciones\n",
    "(interpolación espacial de lo limites exactos geograficos y media espcial ponderada por coseno de la latitud)\n",
    "con el objetivo de verificar cuantitativamente que las diferencias introducidas son despreciables.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151db975-cc38-4b31-b37e-74855ffa74b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lon_to_180(lon):\n",
    "    lon = np.array(lon, dtype=float)\n",
    "    return ((lon + 180) % 360) - 180\n",
    "\n",
    "def _ensure_increasing(lat, lon, field2d):\n",
    "    lat = np.array(lat); lon = np.array(lon)\n",
    "    if lat[0] > lat[-1]:\n",
    "        lat = lat[::-1]\n",
    "        field2d = field2d[::-1, :]\n",
    "    lon_sort_idx = np.argsort(lon)\n",
    "    lon = lon[lon_sort_idx]\n",
    "    field2d = field2d[:, lon_sort_idx]\n",
    "    return lat, lon, field2d\n",
    "\n",
    "def interp2d_to_target(lat_src, lon_src, field2d, lat_t, lon_t):\n",
    "    lat_src, lon_src, field2d = _ensure_increasing(lat_src, lon_src, field2d)\n",
    "    interp = RegularGridInterpolator((lat_src, lon_src), field2d, method=\"linear\",\n",
    "                                    bounds_error=False, fill_value=np.nan)\n",
    "    LonT, LatT = np.meshgrid(lon_t, lat_t)\n",
    "    pts = np.column_stack([LatT.ravel(), LonT.ravel()])\n",
    "    return interp(pts).reshape(len(lat_t), len(lon_t))\n",
    "\n",
    "def area_weighted_mean(field2d, lat_t):\n",
    "    w = np.cos(np.deg2rad(lat_t))[:, None]\n",
    "    m = np.isfinite(field2d)\n",
    "    return np.nan if not np.any(m) else np.nansum(field2d * w) / np.nansum(w * m)\n",
    "\n",
    "# --- Longitudes coherentes ---\n",
    "lon_a_180 = lon_to_180(lon_a)\n",
    "lon_b_180 = lon_to_180(lon_b)\n",
    "\n",
    "years = np.arange(1960, 2021, 1)\n",
    "ddeg = 0.25\n",
    "\n",
    "def series_box(lat_min, lat_max, lon_min, lon_max):\n",
    "    lat_t = np.arange(lat_min, lat_max + 1e-9, ddeg)\n",
    "    lon_t = np.arange(lon_min, lon_max + 1e-9, ddeg)\n",
    "    A = np.empty(61); B = np.empty(61)\n",
    "    for i in range(61):\n",
    "        fld_a = interp2d_to_target(lat_a, lon_a_180, sst_a_annual[i, :, :], lat_t, lon_t)\n",
    "        fld_b = interp2d_to_target(lat_b, lon_b_180, sst_b_annual[i, :, :], lat_t, lon_t)\n",
    "        A[i] = area_weighted_mean(fld_a, lat_t)\n",
    "        B[i] = area_weighted_mean(fld_b, lat_t)\n",
    "    return A, B\n",
    "\n",
    "def trend(y):\n",
    "    c = np.polyfit(years, y, 1)\n",
    "    return np.polyval(c, years), c[0] * 10\n",
    "\n",
    "# ATL3: 4S–4N, 20W–0  (en -180..180)\n",
    "Temp_ATL3_a_intr, Temp_ATL3_b_intr = series_box(-4, 4, -20, 0)\n",
    "\n",
    "# Niño 3.4 (tus coordenadas): 6S–6N, 170W–120W  -> -170 .. -120\n",
    "Temp_Niño_a_intr, Temp_Niño_b_intr = series_box(-6, 6, -170, -120)\n",
    "\n",
    "trend_ATL3_a, slope_ATL3_a_dec = trend(Temp_ATL3_a_intr)\n",
    "trend_ATL3_b, slope_ATL3_b_dec = trend(Temp_ATL3_b_intr)\n",
    "trend_Niño_a, slope_Niño_a_dec = trend(Temp_Niño_a_intr)\n",
    "trend_Niño_b, slope_Niño_b_dec = trend(Temp_Niño_b_intr)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 5), sharex=True)\n",
    "\n",
    "# --- ATL3 ---\n",
    "axs[0].plot(years, Temp_ATL3_a_intr, 'o-', label='HadISST interp (datos)', color='C0', alpha=0.4)\n",
    "axs[0].plot(years, Temp_ATL3_b_intr, 'o-', label='ERSST interp (datos)', color='C1', alpha=0.4)\n",
    "axs[0].plot(years, trend_ATL3_a, label='HadISST interp – Tendencia', color='C0', lw=2.5)\n",
    "axs[0].plot(years, trend_ATL3_b, label='ERSST interp – Tendencia', color='C1', lw=2.5)\n",
    "axs[0].set_title('Atlantic Niño (ATL3) – Interpolado')\n",
    "axs[0].set_ylabel('SST Media Anual (°C)')\n",
    "axs[0].grid(True, alpha=0.3)\n",
    "axs[0].text(0.02, 0.95,\n",
    "            f\"HadISST: {slope_ATL3_a_dec:.2f} °C/década\\nERSST:  {slope_ATL3_b_dec:.2f} °C/década\",\n",
    "            transform=axs[0].transAxes, fontsize=10, va=\"top\")\n",
    "\n",
    "# --- Niño 3.4 ---\n",
    "axs[1].plot(years, Temp_Niño_a_intr, 'o-', label='HadISST interp (datos)', color='C0', alpha=0.4)\n",
    "axs[1].plot(years, Temp_Niño_b_intr, 'o-', label='ERSST interp (datos)', color='C1', alpha=0.4)\n",
    "axs[1].plot(years, trend_Niño_a, label='HadISST interp – Tendencia', color='C0', lw=2.5)\n",
    "axs[1].plot(years, trend_Niño_b, label='ERSST interp – Tendencia', color='C1', lw=2.5)\n",
    "axs[1].set_title('Pacific Niño (Niño 3.4) – Interpolado')\n",
    "axs[1].grid(True, alpha=0.3)\n",
    "axs[1].text(0.02, 0.95,\n",
    "            f\"HadISST: {slope_Niño_a_dec:.2f} °C/década\\nERSST:  {slope_Niño_b_dec:.2f} °C/década\",\n",
    "            transform=axs[1].transAxes, fontsize=10, va=\"top\")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xlabel('Años (1960–2020)')\n",
    "    ax.set_xlim(1960, 2020)\n",
    "    ax.set_xticks(np.arange(1960, 2021, 5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9968c11-cf45-4b7f-9b2f-7a2c4d6cdfd5",
   "metadata": {},
   "source": [
    "![SST_mean_interpoled](img/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e842c4-181e-4845-97d0-1c6397c8723f",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "background-color:#f7f7f7;\n",
    "border-left:5px solid #444;\n",
    "padding:14px;\n",
    "border-radius:4px;\n",
    "line-height:1.5;\n",
    "\">\n",
    "<b>Nota metodológica.</b><br><br>\n",
    "\n",
    "Como ya habíamos anticipado, la interpolación espacial no es estrictamente necesaria para las regiones de estudio consideradas. El área efectiva de cada celda depende del coseno de la latitud; dado que nuestras regiones se extienden como máximo hasta ±5°, este factor toma valores cercanos a 0.996. En consecuencia, el error relativo introducido por no ponderar el área es inferior al 0.4 % incluso en el peor de los casos.\n",
    "\n",
    "Este efecto no altera las tendencias estimadas y únicamente introduce diferencias muy pequeñas en los valores máximos, siempre inferiores a 0.1 °C. Aun así, y por consistencia metodológica, emplearemos definiciones de media ponderada por área y mantendremos el mismo criterio de selección de índices utilizado anteriormente.\n",
    "\n",
    "La selección directa por índices resulta más sencilla y robusta, ya que evita depender de la estructura exacta de las coordenadas originales de cada conjunto de datos. Para homogeneizar todos los productos, utilizaremos la función <code>interp2d_to_target</code> para interpolar los campos a una malla común con las siguientes características:\n",
    "\n",
    "<ul style=\"margin-top:8px;\">\n",
    "  <li>Longitud: −180° a 180°</li>\n",
    "  <li>Latitud: −90° a 90°</li>\n",
    "  <li>Resolución espacial: <code>ddeg = 0.25°</code></li>\n",
    "</ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22929a86-fd0a-4dc2-95f0-47dd38892e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefinimos \n",
    "sst_a = nc1.variables['sst'][:732] #HadISST: R=1°x1°\n",
    "sst_b = nc2.variables['sst'][:732] #ERSST:  R=2°x2°   \n",
    "\n",
    "def to_float_nan(a):\n",
    "    \"\"\"Convierte Variable/MaskedArray a ndarray float con NaNs donde haya máscara.\"\"\"\n",
    "    if np.ma.isMaskedArray(a):\n",
    "        return np.ma.filled(a, np.nan).astype(float)\n",
    "    return np.array(a, dtype=float)\n",
    "\n",
    "def calc_monthly_anoms(fen, detrend=True):\n",
    "    \"\"\"\n",
    "    fen: (time, lat, lon) mensual\n",
    "    return: anoms (year, month, lat, lon)\n",
    "    \"\"\"\n",
    "    arr = to_float_nan(fen)\n",
    "\n",
    "    # Si viniera 4D (time, level, lat, lon), toma level 0\n",
    "    if arr.ndim == 4:\n",
    "        arr = arr[:, 0, :, :]\n",
    "    if arr.ndim != 3:\n",
    "        raise ValueError(f\"Se esperaba (time, lat, lon), pero llegó {arr.shape}\")\n",
    "\n",
    "    nt, ny, nx = arr.shape\n",
    "    nyr = nt // 12\n",
    "    nt_use = nyr * 12\n",
    "    dat = arr[:nt_use].copy()\n",
    "\n",
    "    # Detrend (estable): ajusta con NaNs rellenados, resta solo donde hay dato\n",
    "    if detrend:\n",
    "        x = np.arange(nt_use)\n",
    "        X = np.column_stack([x, np.ones(nt_use)])          # (nt, 2)\n",
    "        Y = dat.reshape(nt_use, -1)                        # (nt, ngrid)\n",
    "        Yf = np.nan_to_num(Y, nan=0.0)                     # para que lstsq no reviente\n",
    "\n",
    "        coeffs = np.linalg.lstsq(X, Yf, rcond=None)[0]     # (2, ngrid)\n",
    "        trend = (X @ coeffs).reshape(dat.shape)            # (nt, ny, nx)\n",
    "\n",
    "        mask = np.isfinite(dat)\n",
    "        dat[mask] = dat[mask] - trend[mask]\n",
    "\n",
    "    # Climatología mensual y anomalías\n",
    "    dat4 = dat.reshape(nyr, 12, ny, nx)                    # (year, month, lat, lon)\n",
    "    clim = np.nanmean(dat4, axis=0)                        # (month, lat, lon)\n",
    "    anoms = dat4 - clim[None, :, :, :]                     # (year, month, lat, lon)\n",
    "    return anoms\n",
    "            \n",
    "#calculem les anomalies dels 4 casos, fem reshape\n",
    "\n",
    "anoms_a0 = calc_monthly_anoms(sst_a, detrend=True)\n",
    "anoms_b = calc_monthly_anoms(sst_b, detrend=True)\n",
    "#en la primera tabla y leyendo la data nos damos cuenta que pcp ya viene en forma de anomalias pero hay que modificar us forma (messes,lat,lon)->(year, month, lat, lon)\n",
    "mes, lat, lon = pcp.shape\n",
    "yr = mes // 12\n",
    "anoms_pcp = pcp.reshape(yr,12,lat,lon)\n",
    "anoms_pcp2 = calc_monthly_anoms(pcp2, detrend=True) #recordar que pcp eran anomalias solo necesitamos calcular las de pcp2\n",
    "\n",
    "def plot_anom_summary(ax, lon, lat, anoms, title, cb_label):\n",
    "    # Campo diagnóstico: máximo absoluto\n",
    "    field = np.nanmax(np.abs(anoms), axis=(0, 1))  # (lat, lon)\n",
    "\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.LAND, facecolor='lightgray')\n",
    "    ax.gridlines(draw_labels=True)\n",
    "\n",
    "    levels = np.linspace(0, 10, 11)\n",
    "\n",
    "    fill = ax.contourf(\n",
    "        lon, lat, field,\n",
    "        levels=levels,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        cmap='YlOrRd',\n",
    "        extend='max'\n",
    "    )\n",
    "\n",
    "    ax.set_title(title, fontsize=12)\n",
    "\n",
    "    cb = plt.colorbar(\n",
    "        fill,\n",
    "        ax=ax,\n",
    "        orientation='horizontal',\n",
    "        pad=0.05,\n",
    "        aspect=40\n",
    "    )\n",
    "    cb.set_label(cb_label, fontsize=12)\n",
    "\n",
    "    # --- mini leyenda min / max ---\n",
    "    vmin = np.nanmin(field)\n",
    "    vmax = np.nanmax(field)\n",
    "\n",
    "    txt = f\"min = {vmin:.2f}\\nmax = {vmax:.2f}\"\n",
    "\n",
    "    ax.text(\n",
    "        0.02, 0.02, txt,\n",
    "        transform=ax.transAxes,\n",
    "        fontsize=9,\n",
    "        verticalalignment='bottom',\n",
    "        horizontalalignment='left',\n",
    "        bbox=dict(\n",
    "            facecolor='white',\n",
    "            edgecolor='none',\n",
    "            alpha=0.75\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return fill\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e88688-45d5-4876-a466-946ecdf3865e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la figura 2×2 para los 4 datasets\n",
    "fig, axes = plt.subplots(\n",
    "    2, 2,\n",
    "    figsize=(16, 12),\n",
    "    subplot_kw={'projection': ccrs.PlateCarree(central_longitude=330)}\n",
    ")\n",
    "\n",
    "plot_anom_summary(\n",
    "    axes[0,0], lon_a, lat_a,\n",
    "    anoms_a0,\n",
    "    'HadISST anomalies: max(|anom|) over 61y×12m',\n",
    "    cb_label='SST anomaly (°C)')\n",
    "\n",
    "plot_anom_summary(\n",
    "    axes[0,1], lon_b, lat_b,\n",
    "    anoms_b,\n",
    "    'ERSST anomalies: max(|anom|) over 61y×12m',\n",
    "    cb_label='SST anomaly (°C)')\n",
    "\n",
    "plot_anom_summary(\n",
    "    axes[1,0], lon_pcp, lat_pcp,\n",
    "    anoms_pcp,\n",
    "    'PCP anomalies: max(|anom|) over 61y×12m',\n",
    "    cb_label='PCP anomaly')\n",
    "\n",
    "plot_anom_summary(\n",
    "    axes[1,1], lon_pcp2, lat_pcp2,\n",
    "    anoms_pcp2,\n",
    "    'PCP2 anomalies: max(|anom|) over 61y×12m',\n",
    "    cb_label='PCP2 anomaly')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda3712b-b7be-4870-a9d2-dc0e6840bcd9",
   "metadata": {},
   "source": [
    "![Data anomalies max](img/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bff251-55da-4b77-9c51-01ff7985c3c4",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "background-color:#f7f7f7;\n",
    "border-left:5px solid #444;\n",
    "padding:14px;\n",
    "border-radius:4px;\n",
    "\">\n",
    "\n",
    "Esta figura se utiliza para evaluar la presencia de valores atípicos (outliers) en las anomalías de temperatura superficial del mar correspondientes a cada base de datos. Se observa que, en el caso de **HadISST**, las regiones polares presentan anomalías extremadamente intensas y físicamente inconsistentes. En particular, aparecen valores máximos de anomalía del orden de **1318 °C**, lo cual carece de sentido físico y evidencia la presencia de errores en esta base de datos que deben ser filtrados previamente al análisis.\n",
    "\n",
    "Por el contrario, en las otras tres bases de datos analizadas no se identifican valores anómalos comparables, mostrando un comportamiento consistente y físicamente razonable en todo el dominio espacial. En este contexto, resulta adecuado aplicar un umbral de filtrado de **±10 °C**, dado que la anomalía máxima observada en un punto a lo largo de todo el periodo analizado es de **9.27 °C**, lo que garantiza la eliminación de valores espurios sin afectar a la señal climática real.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121b8fcc-ca8e-43f5-bf2c-a7ed4bd318ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "anoms_a = anoms_a0.copy()\n",
    "bad_phys = np.abs(anoms_a0) > 10\n",
    "anoms_a[bad_phys] = np.nan #eliminamos los valores que de desvian muchissimo de lo esperado para evitar outliers en anomalias, y con esto tenemos las anomalias limpias\n",
    "\n",
    "# frecuencia absoluta: número de meses problemáticos\n",
    "thr = 10\n",
    "freq = np.sum(np.abs(anoms_a0) > thr, axis=(0, 1))  # (lat, lon)\n",
    "n_total = anoms_a0.shape[0] * anoms_a0.shape[1]  # 61*12 = 732\n",
    "freq_rel = freq / n_total\n",
    "\n",
    "freq_c = np.sum(np.abs(anoms_a) > thr, axis=(0, 1))  # (lat, lon)\n",
    "n_total = anoms_a.shape[0] * anoms_a.shape[1]  # 61*12 = 732\n",
    "freq_rel_c = freq_c / n_total\n",
    "\n",
    "def plot_freq(ax, lon, lat, freq_field, title, vmax=None):\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.LAND, facecolor='lightgray')\n",
    "    ax.gridlines(draw_labels=True)\n",
    "\n",
    "    if vmax is None:\n",
    "        vmax = np.nanmax(freq_field)\n",
    "    levels = np.linspace(0, vmax, 11)\n",
    "\n",
    "    fill = ax.contourf(\n",
    "        lon, lat, freq_field,\n",
    "        levels=levels,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        cmap='Reds',\n",
    "        extend='max'\n",
    "    )\n",
    "\n",
    "    ax.set_title(title, fontsize=12)\n",
    "\n",
    "    cb = plt.colorbar(\n",
    "        fill,\n",
    "        ax=ax,\n",
    "        orientation='horizontal',\n",
    "        pad=0.05,\n",
    "        aspect=40\n",
    "    )\n",
    "    cb.set_label('Number of months', fontsize=12)\n",
    "\n",
    "    # mini-leyenda min/max\n",
    "    vmin = np.nanmin(freq_field)\n",
    "    vmax_f = np.nanmax(freq_field)\n",
    "    n_total = 61 * 12  # meses\n",
    "    freq_rel_global = np.nanmean(freq_field) / n_total\n",
    "\n",
    "    txt = (\n",
    "        f\"min = {vmin:.0f}\\n\"\n",
    "        f\"max = {vmax_f:.0f}\\n\"\n",
    "        f\"mean freq = {100*freq_rel_global:.2f}%\"\n",
    "    )\n",
    "    ax.text(\n",
    "        0.02, 0.02, txt,\n",
    "        transform=ax.transAxes,\n",
    "        fontsize=9,\n",
    "        va='bottom', ha='left',\n",
    "        bbox=dict(facecolor='white', edgecolor='none', alpha=0.75)\n",
    "    )\n",
    "\n",
    "    return fill\n",
    "\n",
    "    # Escala común para comparar\n",
    "vmax_common = np.nanpercentile(np.maximum(freq, freq_c), 99)\n",
    "if vmax_common < 1:\n",
    "    vmax_common = np.nanmax(np.maximum(freq, freq_c))\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    1, 2,\n",
    "    figsize=(16, 5),\n",
    "    subplot_kw={'projection': ccrs.PlateCarree(central_longitude=330)}\n",
    ")\n",
    "\n",
    "plot_freq(\n",
    "    axes[0], lon_a, lat_a,\n",
    "    freq,\n",
    "    f'HadISST frequency: |anom| > {thr}°C (raw)',\n",
    "    vmax=vmax_common\n",
    ")\n",
    "\n",
    "plot_freq(\n",
    "    axes[1], lon_a, lat_a,\n",
    "    freq_c,\n",
    "    f'HadISST frequency: |anom| > {thr}°C (cleaned)',\n",
    "    vmax=vmax_common\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8331222c-ccc0-4f0e-84ab-aad976805404",
   "metadata": {},
   "source": [
    "![Clean Outliers Haddist](img/4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57c698b-fb7d-468b-8484-dc94f372636b",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "background-color:#f7f7f7;\n",
    "border-left:5px solid #444;\n",
    "padding:14px;\n",
    "border-radius:4px;\n",
    "\">\n",
    "\n",
    "En este paso se aplica un filtrado global que elimina las anomalías con valores absolutos superiores a **±10 °C**. Este umbral se adopta en base a criterios físicos y empíricos, dado que la anomalía máxima observada en un punto a lo largo de todo el periodo analizado es de **9.27 °C**. De este modo, el filtrado elimina valores espurios asociados a errores en los datos sin afectar a la señal climática real.\n",
    "\n",
    "La aplicación de este criterio reduce de forma significativa la presencia de anomalías extremas físicamente inconsistentes, permitiendo obtener mapas espaciales coherentes y comparables entre las distintas bases de datos analizadas.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee7ea1c-f8f5-4c37-8e94-9e0a82d20e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_mensual(var_mon, lat_src, lon_src, lat_min, lat_max, lon_min, lon_max,\n",
    "                      ddeg=0.25, reducer=\"area_mean\"):\n",
    "    lon_src_180 = lon_to_180(lon_src)\n",
    "\n",
    "    lat_t = np.arange(lat_min, lat_max + 1e-9, ddeg)\n",
    "    lon_t = np.arange(lon_min, lon_max + 1e-9, ddeg)\n",
    "\n",
    "    out = np.empty(12, dtype=float)\n",
    "\n",
    "    for m in range(12):\n",
    "        fld = interp2d_to_target(lat_src, lon_src_180, var_mon[m, :, :], lat_t, lon_t)\n",
    "\n",
    "        if reducer == \"area_mean\":\n",
    "            out[m] = area_weighted_mean(fld, lat_t)\n",
    "        elif reducer == \"mean\":\n",
    "            out[m] = np.nanmean(fld)\n",
    "        else:\n",
    "            raise ValueError(\"reducer must be 'area_mean' or 'mean'\")\n",
    "\n",
    "    return out\n",
    "\n",
    "# std mensual ya calculada\n",
    "var_a = np.nanstd(anoms_a, axis=0)      # (12, lat, lon)\n",
    "var_b = np.nanstd(anoms_b, axis=0)\n",
    "var_pcp = np.nanstd(anoms_pcp, axis=0)\n",
    "var_pcp2 = np.nanstd(anoms_pcp2, axis=0)\n",
    "\n",
    "variacion_mensual_ATL3_a  = var_mensual(var_a, lat_a, lon_a, -4,  4,  -20,    0, ddeg=0.25)\n",
    "variacion_mensual_Niño_a  = var_mensual(var_a, lat_a, lon_a, -6,  6, -170, -120, ddeg=0.25)\n",
    "\n",
    "variacion_mensual_ATL3_b  = var_mensual(var_b, lat_b, lon_b, -4,  4,  -20,    0, ddeg=0.25)\n",
    "variacion_mensual_Niño_b  = var_mensual(var_b, lat_b, lon_b, -6,  6, -170, -120, ddeg=0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5d16c9-9642-4ed8-86ae-17977987d5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar el gráfico\n",
    "\n",
    "meses = ['Jan','Feb','Mar','Abr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\n",
    "\n",
    "\n",
    "# SSTxATL3 y STTxNiño3.4\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "axs[0].plot(meses, variacion_mensual_ATL3_a,'-ro', markersize=2)\n",
    "axs[0].plot(meses, variacion_mensual_ATL3_b,'-bo', markersize=2)\n",
    "axs[0].grid() # Agrega un fondo cuadriculado al gráfico\n",
    "axs[0].set_ylabel('Variability (°C)') # Renombra el eje y como \"Variability (°C)\"\n",
    "axs[0].set_title('ATL3') \n",
    "axs[0].set_yticks(np.linspace(0.3, 0.7, 12)) # Establece 9 divisiones en el eje y\n",
    "axs[0].yaxis.set_major_formatter(ticker.FormatStrFormatter('%.3f')) # Formatea el eje y para mostrar solo 3 decimales\n",
    "axs[0].margins(x=0.0909)\n",
    "axs[0].legend(['HadISST','ERSST'])\n",
    "\n",
    "\n",
    "axs[1].plot(meses, variacion_mensual_Niño_a,'-ro', markersize=2)\n",
    "axs[1].plot(meses, variacion_mensual_Niño_b,'-bo', markersize=2)\n",
    "axs[1].grid() # Agrega un fondo cuadriculado al gráfico\n",
    "axs[1].set_ylabel('Variability (°C)') # Renombra el eje y como \"Variability (°C)\"\n",
    "axs[1].set_title('Niño3.4') \n",
    "axs[1].set_yticks(np.linspace(0.5, 1.2, 12)) # Establece 9 divisiones en el eje y\n",
    "axs[1].yaxis.set_major_formatter(ticker.FormatStrFormatter('%.3f')) # Formatea el eje y para mostrar solo 3 decimales\n",
    "axs[1].margins(x=0.0909)\n",
    "axs[1].legend(['HadISST','ERSST'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ed209b-40ee-4ab1-883a-ebaba217514b",
   "metadata": {},
   "source": [
    "![Variabilidad Anom_SST](img/5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52cf12c-0188-4658-80a4-a65a63943f2d",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "background-color:#f7f7f7;\n",
    "border-left:5px solid #444;\n",
    "padding:14px;\n",
    "border-radius:4px;\n",
    "\">\n",
    "\n",
    "<b>Variabilidad media mensual de las SST (61 años)</b><br><br>\n",
    "Aquí se muestra la variabilidad media de las anomalías de SST calculada a partir de 61 años de datos,\n",
    "para cada mes del año, en las regiones <b>ATL3</b> y <b>Niño3.4</b>, comparando las bases de datos \n",
    "<span style=\"color:#d62728;\"><b>HadISST</b></span> y \n",
    "<span style=\"color:#1f77b4;\"><b>ERSST</b></span>.\n",
    "\n",
    "Esta variabilidad identifica los periodos del año en los que los fenómenos océano-atmosféricos\n",
    "presentan mayor intensidad:\n",
    "\n",
    "<ul>\n",
    "<li>\n",
    "<b>Atlantic Niño (ATL3):</b>\n",
    "máxima actividad durante el verano boreal, con picos en\n",
    "<b>junio y julio</b>.\n",
    "</li>\n",
    "\n",
    "<li>\n",
    "<b>Pacific Niño (Niño3.4):</b>\n",
    "máxima actividad durante el invierno boreal, con picos en\n",
    "<b>diciembre y enero</b>.\n",
    "</li>\n",
    "</ul>\n",
    "\n",
    "Las diferencias entre ambas bases de datos se encuentran dentro de lo esperado.\n",
    "En el caso de ATL3, \n",
    "<span style=\"color:#d62728;\">HadISST</span> presenta picos y mínimos ligeramente más elevados\n",
    "que <span style=\"color:#1f77b4;\">ERSST</span>.\n",
    "\n",
    "La separación <b>relativa</b> entre \n",
    "<span style=\"color:#d62728;\">HadISST</span> y \n",
    "<span style=\"color:#1f77b4;\">ERSST</span>\n",
    "es mayor en el Atlantic Niño, lo que se aprecia visualmente en el gráfico.\n",
    "En contraste, el Niño del Pacífico muestra una señal mucho más similar entre ambas bases de datos.\n",
    "\n",
    "\n",
    "No obstante, en términos absolutos, las diferencias entre HadISST y ERSST son comparables en ambos\n",
    "casos (≈ 0.05 °C). Por tanto, ATL3 no es más inconsistente en terminos absolutos, es solo que el mismo sesgo de datos\n",
    "tiene un mayor peso relativo cuando la variabilidad climática es menor.\n",
    "\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7d8e7c-dfcf-471e-9bde-53ec0d0516e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "variacion_mensual_ATL3_pcp  = var_mensual(var_pcp, lat_pcp, lon_pcp, -4,  4,  -20,    0, ddeg=0.25)\n",
    "variacion_mensual_Niño_pcp  = var_mensual(var_pcp, lat_pcp, lon_pcp, -6,  6, -170, -120, ddeg=0.25)\n",
    "\n",
    "variacion_mensual_ATL3_pcp2  = var_mensual(var_pcp2, lat_pcp2, lon_pcp2, -4,  4,  -20,    0, ddeg=0.25)\n",
    "variacion_mensual_Niño_pcp2  = var_mensual(var_pcp2, lat_pcp2, lon_pcp2, -6,  6, -170, -120, ddeg=0.25)\n",
    "\n",
    "# SSTxATL3 y STTxNiño3.4\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "axs[0].plot(meses, variacion_mensual_ATL3_pcp,'-ro', markersize=2)\n",
    "axs[0].plot(meses, variacion_mensual_ATL3_pcp2,'-bo', markersize=2)\n",
    "axs[0].grid() # Agrega un fondo cuadriculado al gráfico\n",
    "axs[0].set_ylabel('Variability (mm/day)') # Renombra el eje y como \"Variability (°C)\"\n",
    "axs[0].set_title('ATL3') \n",
    "axs[0].set_yticks(np.linspace(0, 2, 12)) # Establece 9 divisiones en el eje y\n",
    "axs[0].yaxis.set_major_formatter(ticker.FormatStrFormatter('%.3f')) # Formatea el eje y para mostrar solo 3 decimales\n",
    "axs[0].margins(x=0.0909)\n",
    "axs[0].legend(['NOAA PCP','NCEP-NCAR'])\n",
    "\n",
    "\n",
    "axs[1].plot(meses, variacion_mensual_Niño_pcp,'-ro', markersize=2)\n",
    "axs[1].plot(meses, variacion_mensual_Niño_pcp2,'-bo', markersize=2)\n",
    "axs[1].grid() # Agrega un fondo cuadriculado al gráfico\n",
    "axs[1].set_ylabel('Variability (mm/day)') # Renombra el eje y como \"Variability (°C)\"\n",
    "axs[1].set_title('Niño3.4') \n",
    "axs[1].set_yticks(np.linspace(0.5, 3, 12)) # Establece 9 divisiones en el eje y\n",
    "axs[1].yaxis.set_major_formatter(ticker.FormatStrFormatter('%.3f')) # Formatea el eje y para mostrar solo 3 decimales\n",
    "axs[1].margins(x=0.0909)\n",
    "axs[1].legend(['NOAA PCP','NCEP-NCAR'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d601288-0b47-4016-8701-ea190646636d",
   "metadata": {},
   "source": [
    "![Variabilidad Anom_PCP](img/6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9228119b-fddf-4706-847d-d78278e3968d",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "background-color:#f7f7f7;\n",
    "border-left:5px solid #444;\n",
    "padding:14px;\n",
    "border-radius:4px;\n",
    "\">\n",
    "A diferencia de la SST, la variabilidad mensual de la precipitación muestra una mayor dependencia del producto utilizado. NOAA PCP y NCEP-NCAR presentan ciclos estacionales coherentes en ATL3 y Niño-3.4, pero con diferencias notables en amplitud y fase, especialmente en el Pacífico ecuatorial. Estas discrepancias son esperables dada la naturaleza altamente no lineal de la precipitación y las limitaciones de los reanálisis antiguos en regiones tropicales.\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603b548d-575f-4170-a9dd-6af2319d3891",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a definir los indices globales como las anomalias entre la variabilidad\n",
    "index_sst_a = anoms_a / var_a        #HadISST\n",
    "index_sst_b = anoms_b / var_b        #ERSST\n",
    "index_pcp = anoms_pcp / var_pcp      #NOAA PCP\n",
    "index_pcp2 = anoms_pcp2 / var_pcp2   #NCEP-NCAR \n",
    "\n",
    "#A partir de estos indices vamos a calcular su media temporal en los meses de mayor intensidad de cada evento, tendremos como resultado matrices 2D delimitadas por las zonas\n",
    "JJ_months = [5,6]\n",
    "ND_months = [10, 11]\n",
    "\n",
    "index_sst_a_JJ = np.nanmean(index_sst_a[:, JJ_months, :, :], axis=1)\n",
    "index_sst_a_ND = np.nanmean(index_sst_a[:, ND_months, :, :], axis=1)\n",
    "\n",
    "index_sst_b_JJ = np.nanmean(index_sst_b[:, JJ_months, :, :], axis=1)\n",
    "index_sst_b_ND = np.nanmean(index_sst_b[:, ND_months, :, :], axis=1)\n",
    "\n",
    "index_pcp_JJ = np.nanmean(index_pcp[:, JJ_months, :, :], axis=1)\n",
    "index_pcp_ND = np.nanmean(index_pcp[:, ND_months, :, :], axis=1)\n",
    "\n",
    "index_pcp2_JJ = np.nanmean(index_pcp2[:, JJ_months, :, :], axis=1)\n",
    "index_pcp2_ND = np.nanmean(index_pcp2[:, ND_months, :, :], axis=1)\n",
    "\n",
    "#Tambien vamos a calcular la media de estos indices en la zona de cada evento.\n",
    "\n",
    "# Regiones (en coordenadas físicas)\n",
    "ATL3_lats = (-4.0, 4.0)\n",
    "ATL3_lons = (-20, 0)      # 20W–0E \n",
    "\n",
    "NIÑO_lats = (-6.0, 6.0)\n",
    "NIÑO_lons = (-170, -120)      # 170W–120W \n",
    "\n",
    "\n",
    "def area_mean_index(season_field, lat, lon, lat_bounds, lon_bounds):\n",
    "    lat = np.asarray(lat)\n",
    "    lon = lon_to_180(lon)\n",
    "\n",
    "    lat_min, lat_max = lat_bounds\n",
    "    lon_min, lon_max = lon_bounds\n",
    "\n",
    "    lat_mask = (lat >= lat_min) & (lat <= lat_max)\n",
    "    lon_mask = (lon >= lon_min) & (lon <= lon_max)\n",
    "\n",
    "    sub = season_field[:, lat_mask, :][:, :, lon_mask]\n",
    "    return np.nanmean(sub, axis=(1, 2))\n",
    "    \n",
    "#Vamos a coger los indices en forma de matriz anteriores y les vamos a calcular la media de temperatura para cada zona especifica, este indice resultantante solo tendra dimension temporal\n",
    "# Hadisst\n",
    "ATL3_sst_a_index = area_mean_index(index_sst_a_JJ, lat_a, lon_a, ATL3_lats, ATL3_lons)\n",
    "Niño_sst_a_index = area_mean_index(index_sst_a_ND, lat_a, lon_a, NIÑO_lats, NIÑO_lons) \n",
    "\n",
    "# ERSST\n",
    "ATL3_sst_b_index = area_mean_index(index_sst_b_JJ, lat_b, lon_b, ATL3_lats, ATL3_lons)\n",
    "Niño_sst_b_index = area_mean_index(index_sst_b_ND, lat_b, lon_b, NIÑO_lats, NIÑO_lons)  \n",
    "    \n",
    "# NOAA PCP\n",
    "ATL3_pcp_index = area_mean_index(index_pcp_JJ, lat_pcp, lon_pcp, ATL3_lats, ATL3_lons)\n",
    "Niño_pcp_index = area_mean_index(index_pcp_ND, lat_pcp, lon_pcp, NIÑO_lats, NIÑO_lons) \n",
    "\n",
    "# #NCEP-NCAR\n",
    "ATL3_pcp2_index = area_mean_index(index_pcp2_JJ, lat_pcp2, lon_pcp2, ATL3_lats, ATL3_lons)\n",
    "Niño_pcp2_index = area_mean_index(index_pcp2_ND, lat_pcp2, lon_pcp2, NIÑO_lats, NIÑO_lons)\n",
    "\n",
    "#Una vez tenemos los indices vamos a calcular la media de anomalies en en los messes de Julio Junio y de Noviembre Diciembre, estas son globales\n",
    "anoms_a_JJ = np.mean(anoms_a[:,JJ_months,:,:], axis=1)\n",
    "anoms_b_JJ = np.mean(anoms_b[:,JJ_months,:,:], axis=1)\n",
    "anoms_pcp_JJ = np.mean(anoms_pcp[:,JJ_months,:,:], axis=1)\n",
    "anoms_pcp2_JJ = np.mean(anoms_pcp2[:,JJ_months,:,:], axis=1)\n",
    "\n",
    "anoms_a_ND = np.mean(anoms_a[:,ND_months,:,:], axis=1)\n",
    "anoms_b_ND = np.mean(anoms_b[:,ND_months,:,:], axis=1)\n",
    "anoms_pcp_ND = np.mean(anoms_pcp[:,ND_months,:,:], axis=1)\n",
    "anoms_pcp2_ND = np.mean(anoms_pcp2[:,ND_months,:,:], axis=1)\n",
    "\n",
    "#Vamos a definir una funcion donde poder ver la tendencia y las correlacion entre estas anomalias en los periodos JJ y ND y su indice normalizado y medio de cada zona  \n",
    "# Analizamos en qué regiones del mundo las anomalías de SST o precipitación\n",
    "# covarían con el índice regional (ATL3 o Niño3.4).\n",
    "# Las zonas con alta correlación indican regiones donde la variabilidad\n",
    "# interanual está fuertemente acoplada al índice, por lo tanto al evento climatico\n",
    "\n",
    "def find_a_r(anom_det, index):\n",
    "    index = np.asarray(index)\n",
    "    n_time = len(index)\n",
    "\n",
    "    x = index\n",
    "    x_mean = np.mean(x)\n",
    "    x_centered = x - x_mean\n",
    "\n",
    "    y = anom_det  # (time, lat, lon)\n",
    "    y_mean = np.mean(y, axis=0, keepdims=True)\n",
    "    y_centered = y - y_mean\n",
    "\n",
    "    cov = np.sum(x_centered[:, np.newaxis, np.newaxis] * y_centered, axis=0) / (n_time - 1)\n",
    "    var_x = np.var(x, ddof=1)\n",
    "    slopes = cov / var_x\n",
    "\n",
    "    std_y = np.std(y, axis=0, ddof=1)\n",
    "    correlations = cov / (np.std(x, ddof=1) * std_y)\n",
    "\n",
    "    return slopes, correlations\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3040ffc5-3ed2-4437-a702-30dab235152b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_HADISST_ATL3, corr_HADISST_ATL3 = find_a_r(anoms_a_JJ, ATL3_sst_a_index)\n",
    "\n",
    "a_HADISST_NIÑO, corr_HADISST_NIÑO = find_a_r(anoms_a_ND, Niño_sst_a_index)\n",
    "\n",
    "a_ERSST_ATL3, corr_ERSST_ATL3 = find_a_r(anoms_b_JJ, ATL3_sst_b_index)\n",
    "\n",
    "a_ERSST_NIÑO, corr_ERSST_NIÑO = find_a_r(anoms_b_ND, Niño_sst_b_index)\n",
    "\n",
    "a_pcp_ATL3, corr_pcp_ATL3 = find_a_r(anoms_pcp_JJ, ATL3_pcp_index)\n",
    "\n",
    "a_pcp_NIÑO, corr_pcp_NIÑO = find_a_r(anoms_pcp_ND, Niño_pcp_index)\n",
    "\n",
    "a_pcp2_ATL3, corr_pcp2_ATL3 = find_a_r(anoms_pcp2_JJ, ATL3_pcp2_index)\n",
    "\n",
    "a_pcp2_NIÑO, corr_pcp2_NIÑO = find_a_r(anoms_pcp2_ND, Niño_pcp2_index)\n",
    "\n",
    "def configure_plot(ax, title, cb_label, data_type='temp'):\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.LAND, facecolor='lightgray')\n",
    "    ax.gridlines(draw_labels=True)\n",
    "\n",
    "    cmap = plt.cm.RdBu_r\n",
    "    clev = np.linspace(-1, 1, 11)\n",
    "\n",
    "    return cmap, clev\n",
    "    \n",
    "def create_plot(ax, lon, lat, slope_data, corr_data, title, cb_label, data_type='temp'):\n",
    "    # --- Constantes del estudio ---\n",
    "    N = 61\n",
    "    alpha = 0.05\n",
    "    df = N - 2\n",
    "\n",
    "    # --- Base map ---\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.LAND, facecolor='lightgray')\n",
    "    ax.gridlines(draw_labels=True)\n",
    "\n",
    "    cmap = plt.cm.RdBu_r\n",
    "\n",
    "    slope_plot = np.array(slope_data, copy=True)\n",
    "    corr_plot  = np.array(corr_data,  copy=True)\n",
    "\n",
    "    # COLORES-SLOPE\n",
    "    if data_type == 'temp':\n",
    "        clev_slope = np.linspace(-1, 1, 11)\n",
    "    else:\n",
    "        vmax = np.nanpercentile(np.abs(slope_plot), 95)\n",
    "        vmax = max(min(vmax, 2.0), 1e-6)\n",
    "        clev_slope = np.linspace(-vmax, vmax, 11)\n",
    "\n",
    "    fill = ax.contourf(\n",
    "        lon, lat, slope_plot,\n",
    "        levels=clev_slope,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        cmap=cmap,\n",
    "        extend='both'\n",
    "    )\n",
    "\n",
    "    \n",
    "    #CONTORNOS = r (tamaño de correlación)\n",
    "    corr_levels = [-0.6, -0.4, -0.2, 0.2, 0.4, 0.6]\n",
    "    cont = ax.contour(lon, lat, corr_plot,levels=corr_levels,transform=ccrs.PlateCarree(),colors='k',linewidths=0.7,alpha=0.7)\n",
    "    ax.clabel(cont, inline=True, fontsize=8, fmt=\"%.1f\")\n",
    "\n",
    "    \n",
    "    #HACHURADO = p < 0.05 (significancia de r)\n",
    "    r = np.clip(corr_plot, -0.999999, 0.999999)\n",
    "    t_stat = r * np.sqrt(df / (1.0 - r**2))\n",
    "    pval = 2.0 * stats.t.sf(np.abs(t_stat), df)\n",
    "    sig = (pval < alpha).astype(int)  # 1 significativo\n",
    "    ax.contourf(lon, lat, sig,levels=[0.5, 1.5],transform=ccrs.PlateCarree(),colors='none',hatches=['....'])  # cambia a '///' si lo prefieres)\n",
    "\n",
    "    # --- Título y colorbar ---\n",
    "    ax.set_title(title, fontsize=12)\n",
    "\n",
    "    cb = plt.colorbar(\n",
    "        fill,\n",
    "        ax=ax,\n",
    "        orientation='horizontal',\n",
    "        pad=0.05,\n",
    "        aspect=40\n",
    "    )\n",
    "    cb.set_label(cb_label, fontsize=12)\n",
    "\n",
    "    return fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020de939-8f14-4bd4-abe6-158becebdbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSTxATL3 y STTxNiño3.4\n",
    "fig, axes = plt.subplots(\n",
    "    2, 2,\n",
    "    figsize=(16, 12),\n",
    "    subplot_kw={'projection': ccrs.PlateCarree(central_longitude=330)}\n",
    ")\n",
    "\n",
    "create_plot(\n",
    "    axes[0,0], lon_a, lat_a,\n",
    "    a_HADISST_ATL3, corr_HADISST_ATL3,\n",
    "    'SSTxALT3 JJ (HADISST)', 'Temperature (°C)'\n",
    ")\n",
    "\n",
    "create_plot(\n",
    "    axes[0,1], lon_a, lat_a,\n",
    "    a_HADISST_NIÑO, corr_HADISST_NIÑO,\n",
    "    'SSTxNiño3.4 ND (HADISST)', 'Temperature (°C)'\n",
    ")\n",
    "\n",
    "create_plot(\n",
    "    axes[1,0], lon_b, lat_b,\n",
    "    a_ERSST_ATL3, corr_ERSST_ATL3,\n",
    "    'SSTxALT3 JJ (ERSST)', 'Temperature (°C)'\n",
    ")\n",
    "\n",
    "create_plot(\n",
    "    axes[1,1], lon_b, lat_b,\n",
    "    a_ERSST_NIÑO, corr_ERSST_NIÑO,\n",
    "    'SSTxNiño3.4 ND (ERSST)', 'Temperature (°C)'\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91a1643-8c0b-4c39-969b-a51817a16ff5",
   "metadata": {},
   "source": [
    "![Correlacion/Slope Anom_SST-index](img/7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c93344-90cf-4435-ac04-7638b6f05bf5",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "background-color:#f7f7f7;\n",
    "border-left:5px solid #444;\n",
    "padding:14px;\n",
    "border-radius:4px;\n",
    "\">\n",
    "En los paneles correspondientes al <b>Atlantic Niño</b> (izquierda, <b>JJ</b>) se observa\n",
    "un calentamiento <b>muy localizado</b> en el <b>Atlántico ecuatorial</b>, claramente\n",
    "<b>centrado en la región ATL3</b>.\n",
    "La amplitud de las anomalías es <b>moderada en términos absolutos</b> y sensiblemente\n",
    "<b>menor que la asociada al ENSO</b>.\n",
    "El patrón espacial es fundamentalmente <b>regional</b> y no domina el resto de las\n",
    "cuencas oceánicas.\n",
    "Las áreas punteadas indican regiones con <b>significancia estadística</b>, aunque su\n",
    "extensión espacial es <b>limitada</b>, lo que refuerza el carácter localizado y estacional\n",
    "del fenómeno.\n",
    "En conjunto, el Atlantic Niño se manifiesta como un evento <b>estacional, regional y de\n",
    "escala espacial reducida</b>, sin evidenciar <b>interconexiones globales robustas</b>.\n",
    "<br><br>\n",
    "\n",
    "Por el contrario, en los paneles correspondientes al <b>Pacific Niño / ENSO</b>\n",
    "(derecha, <b>ND</b>) se aprecia un calentamiento <b>mucho más intenso</b> en el\n",
    "<b>Pacífico ecuatorial</b>, caracterizado por la presencia de la <b>típica lengua cálida\n",
    "ecuatorial</b>.\n",
    "La señal se <b>extiende ampliamente</b> a lo largo del Pacífico tropical y presenta\n",
    "<b>impactos claros en otras cuencas oceánicas</b>.\n",
    "El patrón espacial es más <b>simétrico zonalmente</b> y claramente\n",
    "<b>dominante a escala global</b>.\n",
    "Las regiones punteadas cubren áreas extensas, confirmando la <b>robustez estadística</b>\n",
    "del patrón asociado al ENSO.\n",
    "En este sentido, el Pacific Niño se identifica como el <b>principal modo de variabilidad\n",
    "interanual tropical</b>, caracterizado por la presencia de <b>interconexiones a escala\n",
    "global</b> que modulan la variabilidad climática en múltiples regiones del planeta.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624a4443-d718-4e69-9bd7-8977669294ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCPxATL3 y PCPxNiño3.4\n",
    "fig, axes = plt.subplots(\n",
    "    2, 2,\n",
    "    figsize=(16, 12),\n",
    "    subplot_kw={'projection': ccrs.PlateCarree(central_longitude=330)}\n",
    ")\n",
    "\n",
    "create_plot(\n",
    "    axes[0,0], lon_pcp, lat_pcp,\n",
    "    a_pcp_ATL3, corr_pcp_ATL3,\n",
    "    'PCPxALT3 JJ (NOAA)', 'Precipitation (mm/day)', 'precip'\n",
    ")\n",
    "\n",
    "create_plot(\n",
    "    axes[0,1], lon_pcp, lat_pcp,\n",
    "    a_pcp_NIÑO, corr_pcp_NIÑO,\n",
    "    'PCPxNiño3.4 ND (NOAA)', 'Precipitation (mm/day)', 'precip'\n",
    ")\n",
    "\n",
    "create_plot(\n",
    "    axes[1,0], lon_pcp2, lat_pcp2,\n",
    "    a_pcp2_ATL3, corr_pcp2_ATL3,\n",
    "    'PCP2xALT3 JJ (NCEP)', 'Precipitation (mm/day)', 'precip'\n",
    ")\n",
    "\n",
    "create_plot(\n",
    "    axes[1,1], lon_pcp2, lat_pcp2,\n",
    "    a_pcp2_NIÑO, corr_pcp2_NIÑO,\n",
    "    'PCP2xNiño3.4 ND (NCEP)', 'Precipitation (mm/day)', 'precip'\n",
    ")\n",
    " \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde53dcd-6478-49d5-a56b-29d6370b3a48",
   "metadata": {},
   "source": [
    "![Correlacion/Slope Anom_PCP-index](img/8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ef65d1-97bf-48aa-ad9c-43483a92d9ce",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "background-color:#f7f7f7;\n",
    "border-left:5px solid #444;\n",
    "padding:14px;\n",
    "border-radius:4px;\n",
    "\">\n",
    "En los paneles correspondientes al <b>Atlantic Niño</b> (izquierda, <b>JJ</b>), tanto en la\n",
    "base de datos de <b>NOAA</b> como en <b>NCEP</b>, se observan anomalías de precipitación\n",
    "<b>concentradas principalmente en el Atlántico ecuatorial</b> y regiones adyacentes.\n",
    "El patrón de lluvia presenta una estructura predominantemente <b>regional</b>, con\n",
    "señales coherentes en la franja ecuatorial atlántica, pero con una\n",
    "<b>extensión espacial limitada</b>.\n",
    "Las áreas punteadas indican regiones con <b>significancia estadística</b>, que se\n",
    "restringen mayoritariamente al Atlántico tropical y zonas continentales próximas.\n",
    "Este comportamiento refuerza la interpretación del Atlantic Niño como un fenómeno\n",
    "<b>acoplado océano–atmósfera de alcance regional</b>.\n",
    "<br><br>\n",
    "\n",
    "No obstante, los mapas muestran que el <b>Atlantic Niño</b> puede inducir\n",
    "<b>respuestas atmosféricas remotas secundarias</b>, visibles en forma de anomalías de\n",
    "precipitación más débiles en otras regiones, como el <b>Pacífico tropical</b>\n",
    "(anomalías negativas), <b>Sudamérica austral</b> y sectores del <b>Índico y Sudeste\n",
    "Asiático</b>.\n",
    "Estas señales presentan una distribución espacial <b>más irregular y fragmentada</b>,\n",
    "con menor coherencia entre regiones y una <b>robustez estadística más limitada</b>,\n",
    "además de una mayor dependencia del evento concreto y de la base de datos utilizada.\n",
    "<br><br>\n",
    "\n",
    "Por el contrario, los paneles correspondientes al <b>Pacific Niño / ENSO</b>\n",
    "(derecha, <b>ND</b>) muestran anomalías de precipitación <b>mucho más intensas y\n",
    "espacialmente extensas</b>, dominadas por la <b>típica lengua húmeda ecuatorial</b> en el\n",
    "Pacífico.\n",
    "Este calentamiento oceánico induce una <b>respuesta atmosférica altamente organizada</b>,\n",
    "con compensaciones claras entre regiones, como el patrón característico de\n",
    "<b>Pacífico central húmedo</b> y <b>regiones de Asia y el Pacífico occidental más secas</b>.\n",
    "Las áreas punteadas cubren <b>amplias regiones del globo</b>, confirmando la\n",
    "<b>robustez estadística</b> y la consistencia del patrón entre datasets.\n",
    "<br><br>\n",
    "\n",
    "Estas diferencias ponen de manifiesto que la distinción clave entre ambos fenómenos\n",
    "<b>no reside en la presencia o ausencia de respuestas remotas</b>, sino en su\n",
    "<b>coherencia, organización y dominancia dinámica</b>.\n",
    "El <b>ENSO</b> genera <b>teleconexiones climáticas robustas y repetibles</b>, asociadas a\n",
    "una <b>reconfiguración clara de la circulación de Walker</b> y a ajustes en la\n",
    "circulación de <b>Hadley</b>, dando lugar a una reorganización global del campo de\n",
    "precipitación.\n",
    "<br><br>\n",
    "\n",
    "El <b>Atlantic Niño</b>, en cambio, puede inducir respuestas remotas, pero estas son\n",
    "<b>más débiles, menos simétricas, menos persistentes</b> y\n",
    "<b>no dominantes frente a la variabilidad interna del sistema climático</b>.\n",
    "Esta diferencia dinámica constituye la <b>justificación física</b> para exigir, en la\n",
    "identificación de eventos de <b>Atlantic Niño</b>, la <b>independencia respecto al ENSO</b>.\n",
    "Incluso cuando se cumplen criterios térmicos y temporales en <b>ATL3</b>, la presencia\n",
    "simultánea de un ENSO activo puede dar lugar a respuestas atmosféricas inducidas,\n",
    "generando <b>falsos positivos</b> si no se considera explícitamente el estado del\n",
    "Pacífico.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55a9c46-20ca-4151-8836-57c745cfaa50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
