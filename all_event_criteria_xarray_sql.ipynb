{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "52a51f5d-ac98-459b-bf99-06602568189d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Librerías Estándar \n",
    "import random\n",
    "import warnings\n",
    "\n",
    "# Análisis de Datos y Matemáticas \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from scipy import stats\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "from netCDF4 import Dataset as ncread  # Asumiendo que usas ncread explícitamente\n",
    "\n",
    "# Visuals\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib import animation\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "#Utilidades y Entorno (Jupyter/IPython) ---\n",
    "from tabulate import tabulate\n",
    "from IPython.display import display, HTML, Image  # Todo en una sola línea\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4119ac95-0ba3-45dc-82c7-ceb569a67395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import sqlite3  # Para trabajar con SQLite\n",
    "import pandas as pd  # Para manejar datos como DataFrames\n",
    "import duckdb  # Para trabajar con DuckDB (base de datos en memoria)\n",
    "import pandas as pd  # Para trabajar con DataFrames\n",
    "import pandas as pd  # Para manejar datos como DataFrames\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "# ---------- Paths ----------\n",
    "path = Path(r\"E:/TFG/Datos/Importants/\")\n",
    "data1 = path / \"HadISST_sst_v1.1_196001_202105.nc\"\n",
    "data2 = path / \"sst.mnmean_v5_196001_202105.nc\"\n",
    "NT = 732  # meses a usar (como en tu notebook)\n",
    "OUT = path / \"_sql_exports\"\n",
    "OUT.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d0a6a3ad-a8af-4974-84aa-767e7557eaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Funciones de carga y procesamiento ----------\n",
    "def to_lat_lon_latitude_longitude(da: xr.DataArray) -> xr.DataArray:\n",
    "    \"\"\"Convierte las coordenadas de latitud y longitud a convenciones estándar.\"\"\"\n",
    "    \n",
    "    # Renombrar \"latitude\" a \"lat\" si existe\n",
    "    if \"latitude\" in da.coords and \"lat\" not in da.coords:\n",
    "        da = da.rename({\"latitude\": \"lat\"})\n",
    "        \n",
    "    # Renombrar \"longitude\" a \"lon\" si existe\n",
    "    if \"longitude\" in da.coords and \"lon\" not in da.coords:\n",
    "        da = da.rename({\"longitude\": \"lon\"})\n",
    "        \n",
    "    # Convierte las longitudes a la convención -180...180\n",
    "    lon = da[\"lon\"]\n",
    "    lon2 = ((lon + 180) % 360) - 180\n",
    "    da = da.assign_coords(lon=lon2).sortby(\"lon\")\n",
    "    \n",
    "    return da\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def open_sst_clean(fp: Path, varname: str, nt: int = NT) -> xr.DataArray:\n",
    "    \"\"\"Abre el dataset NetCDF y limpia con xarray.\"\"\"\n",
    "    ds = xr.open_dataset(fp, decode_cf=True, mask_and_scale=True)\n",
    "    ds = to_lat_lon_latitude_longitude(ds)\n",
    "    if varname not in ds:\n",
    "        raise KeyError(f\"{varname} no está en {fp.name}. vars={list(ds.data_vars)}\")\n",
    "    \n",
    "    \n",
    "    # Seleccionamos la variable de interés (por ejemplo, \"sst\") y recortamos el tiempo\n",
    "    da = ds[varname].isel(time=slice(0, nt))\n",
    "\n",
    "    # Aplicamos el filtro de valores válidos para SST\n",
    "    da = da.where((da >= -1.79) & (da <= 35))  # Reemplazar valores fuera del rango con NaN\n",
    "\n",
    "    # Aseguramos que las longitudes estén en la convención -180 a 180\n",
    "    return da\n",
    "\n",
    "\n",
    "# Abre el Dataset completo para cada archivo\n",
    "\n",
    "\n",
    "def to_float_nan(a):\n",
    "    \"\"\"Convierte Variable/MaskedArray a ndarray float con NaNs donde haya máscara.\"\"\"\n",
    "    if np.ma.isMaskedArray(a):\n",
    "        return np.ma.filled(a, np.nan).astype(float)\n",
    "    return np.array(a, dtype=float)\n",
    "\n",
    "def calc_monthly_anoms(fen, detrend=True):\n",
    "    \"\"\"\n",
    "    fen: (time, lat, lon) mensual (DataArray)\n",
    "    ds: Dataset completo (contiene las coordenadas)\n",
    "    return: anoms (year, month, lat, lon)\n",
    "    \"\"\"\n",
    "    arr = to_float_nan(fen)  # Aseguramos que los valores NaN se convierten a np.nan\n",
    "\n",
    "\n",
    "    # Si viniera 4D (time, level, lat, lon), toma level 0\n",
    "    if arr.ndim == 4:\n",
    "        arr = arr[:, 0, :, :]\n",
    "    if arr.ndim != 3:\n",
    "        raise ValueError(f\"Se esperaba (time, lat, lon), pero llegó {arr.shape}\")\n",
    "\n",
    "    nt, ny, nx = arr.shape\n",
    "    nyr = nt // 12\n",
    "    nt_use = nyr * 12\n",
    "    dat = arr[:nt_use].copy()\n",
    "\n",
    "    # Detrend (ajustamos la tendencia temporal si se pide)\n",
    "    if detrend:\n",
    "        x = np.arange(nt_use)\n",
    "        X = np.column_stack([x, np.ones(nt_use)])  # Matriz de diseño (tiempo, 1) para la regresión\n",
    "        Y = dat.reshape(nt_use, -1)  # Aplanamos el array para usar mínimos cuadrados\n",
    "        Yf = np.nan_to_num(Y, nan=0.0)  # Convertir NaNs a 0 para la regresión\n",
    "\n",
    "        coeffs = np.linalg.lstsq(X, Yf, rcond=None)[0]  # Resuelve la regresión lineal\n",
    "        trend = (X @ coeffs).reshape(dat.shape)  # Calcula la tendencia en el tiempo\n",
    "\n",
    "        mask = np.isfinite(dat)  # Creamos una máscara de los valores no nulos\n",
    "        dat[mask] = dat[mask] - trend[mask]  # Restamos la tendencia a los datos no nulos\n",
    "\n",
    "    # Reorganizar datos a (año, mes, lat, lon)\n",
    "    dat4 = dat.reshape(nyr, 12, ny, nx)  # (year, month, lat, lon)\n",
    "\n",
    "    # Calcular la climatología mensual (media de cada mes a lo largo de los años)\n",
    "    clim = np.nanmean(dat4, axis=0)  # Calculamos la climatología mensual\n",
    "\n",
    "    # Anomalías: restamos la climatología mensual\n",
    "    anoms = dat4 - clim[None, :, :, :]  # (year, month, lat, lon)\n",
    "\n",
    "    # Acceder a las coordenadas lat y lon directamente\n",
    "    lat = fen.coords[\"lat\"]\n",
    "    lon = fen.coords[\"lon\"]\n",
    "\n",
    "    # Devolvemos el DataArray con las anomalías\n",
    "    return xr.DataArray(\n",
    "        anoms,\n",
    "        dims=[\"year\", \"month\", \"lat\", \"lon\"],  # Usamos \"lat\" y \"lon\" directamente\n",
    "        coords={\n",
    "            \"year\": np.arange(nyr),\n",
    "            \"month\": np.arange(12),\n",
    "            \"lat\": lat,  # Usamos las coordenadas de latitud\n",
    "            \"lon\": lon,  # Usamos las coordenadas de longitud\n",
    "        },\n",
    "    )\n",
    "\n",
    "def region_mean(anoms: xr.DataArray, region: dict) -> xr.DataArray:\n",
    "    \"\"\"Calcula la media de las anomalías para una región específica.\"\"\"\n",
    "    # Seleccionar la región usando las coordenadas de latitud y longitud\n",
    "    anoms_region = anoms.sel(lat=region[\"lat\"], lon=region[\"lon\"])  # Seleccionamos la región\n",
    "\n",
    "    # Calculamos la media espacial sobre las dimensiones de latitud y longitud\n",
    "    return anoms_region.mean(dim=[\"lat\", \"lon\"], skipna=True)  # Media sobre latitudes y longitudes\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0061d4d4-3d74-4088-81c1-b0638daddfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    year  month   anomaly  region       source\n",
      "0   1960      1  0.121873  Nino34  HadISST_sst\n",
      "1   1960      2 -0.139709  Nino34  HadISST_sst\n",
      "2   1960      3  0.051061  Nino34  HadISST_sst\n",
      "3   1960      4  0.143723  Nino34  HadISST_sst\n",
      "4   1960      5  0.209010  Nino34  HadISST_sst\n",
      "5   1960      6 -0.082081  Nino34  HadISST_sst\n",
      "6   1960      7 -0.109685  Nino34  HadISST_sst\n",
      "7   1960      8  0.114966  Nino34  HadISST_sst\n",
      "8   1960      9  0.177948  Nino34  HadISST_sst\n",
      "9   1960     10  0.042939  Nino34  HadISST_sst\n",
      "10  1960     11 -0.268896  Nino34  HadISST_sst\n",
      "11  1960     12  0.115227  Nino34  HadISST_sst\n",
      "12  1961      1 -0.019849  Nino34  HadISST_sst\n",
      "13  1961      2  0.140307  Nino34  HadISST_sst\n",
      "14  1961      3  0.041600  Nino34  HadISST_sst\n",
      "15  1961      4  0.227473  Nino34  HadISST_sst\n",
      "16  1961      5  0.118074  Nino34  HadISST_sst\n",
      "17  1961      6  0.184192  Nino34  HadISST_sst\n",
      "18  1961      7 -0.205068  Nino34  HadISST_sst\n",
      "19  1961      8 -0.153542  Nino34  HadISST_sst\n",
      "20  1961      9 -0.376150  Nino34  HadISST_sst\n",
      "21  1961     10 -0.599533  Nino34  HadISST_sst\n",
      "22  1961     11 -0.129624  Nino34  HadISST_sst\n",
      "23  1961     12 -0.225395  Nino34  HadISST_sst\n",
      "24  1962      1 -0.127364  Nino34  HadISST_sst\n",
      "25  1962      2 -0.060435  Nino34  HadISST_sst\n",
      "26  1962      3 -0.252840  Nino34  HadISST_sst\n",
      "27  1962      4 -0.211210  Nino34  HadISST_sst\n",
      "28  1962      5 -0.283365  Nino34  HadISST_sst\n",
      "29  1962      6 -0.134644  Nino34  HadISST_sst\n",
      "30  1962      7 -0.186374  Nino34  HadISST_sst\n",
      "31  1962      8 -0.033484  Nino34  HadISST_sst\n",
      "32  1962      9 -0.414506  Nino34  HadISST_sst\n",
      "33  1962     10 -0.247678  Nino34  HadISST_sst\n",
      "34  1962     11 -0.346551  Nino34  HadISST_sst\n",
      "35  1962     12 -0.411222  Nino34  HadISST_sst\n",
      "36  1963      1 -0.195029  Nino34  HadISST_sst\n",
      "37  1963      2 -0.145543  Nino34  HadISST_sst\n",
      "38  1963      3  0.076584  Nino34  HadISST_sst\n",
      "39  1963      4  0.068526  Nino34  HadISST_sst\n",
      "Verificación de NaNs en las anomalías:\n",
      "year       0\n",
      "month      0\n",
      "anomaly    0\n",
      "region     0\n",
      "source     0\n",
      "dtype: int64\n",
      "OK -> E:\\TFG\\Datos\\Importants\\_sql_exports\\regions_timeseries_anomalies_detrended.parquet\n"
     ]
    }
   ],
   "source": [
    "# Cargar los datos y calcular las anomalías mensuales\n",
    "had_sst = open_sst_clean(data1, \"sst\", NT)\n",
    "ers_sst = open_sst_clean(data2, \"sst\", NT)\n",
    "\n",
    "# Seleccionar las regiones Niño3.4 y ATL3 y calcular anomalías\n",
    "NINO34 = dict(lat=slice(6, -6), lon=slice(-170, -120))\n",
    "ATL3 = dict(lat=slice(4, -4), lon=slice(-20, 0))\n",
    "\n",
    "# Calcular las anomalías mensuales para Niño3.4 y ATL3\n",
    "sst_nino34_anoms = calc_monthly_anoms(had_sst)\n",
    "sst_atl3_anoms = calc_monthly_anoms(ers_sst)\n",
    "\n",
    "# Calcular la media por región para Niño 3.4 en ambos datasets\n",
    "nino34_mean_had = region_mean(sst_nino34_anoms, NINO34)\n",
    "nino34_mean_ers = region_mean(sst_atl3_anoms, NINO34)\n",
    "\n",
    "# Calcular la media por región para ATL3 en ambos datasets\n",
    "atl3_mean_had = region_mean(sst_nino34_anoms, ATL3)\n",
    "atl3_mean_ers = region_mean(sst_atl3_anoms, ATL3)\n",
    "\n",
    "# Convertir a DataFrame y añadir etiquetas\n",
    "nino34_df_had = nino34_mean_had.to_dataframe(name=\"anomaly\").reset_index()\n",
    "nino34_df_had[\"region\"] = \"Nino34\"\n",
    "nino34_df_had[\"source\"] = \"HadISST_sst\"\n",
    "\n",
    "nino34_df_ers = nino34_mean_ers.to_dataframe(name=\"anomaly\").reset_index()\n",
    "nino34_df_ers[\"region\"] = \"Nino34\"\n",
    "nino34_df_ers[\"source\"] = \"ERSST_sst\"\n",
    "\n",
    "atl3_df_had = atl3_mean_had.to_dataframe(name=\"anomaly\").reset_index()\n",
    "atl3_df_had[\"region\"] = \"ATL3\"\n",
    "atl3_df_had[\"source\"] = \"HadISST_sst\"\n",
    "\n",
    "atl3_df_ers = atl3_mean_ers.to_dataframe(name=\"anomaly\").reset_index()\n",
    "atl3_df_ers[\"region\"] = \"ATL3\"\n",
    "atl3_df_ers[\"source\"] = \"ERSST_sst\"\n",
    "\n",
    "# Unir todas las tablas\n",
    "final_table = pd.concat([nino34_df_had, nino34_df_ers, atl3_df_had, atl3_df_ers], ignore_index=True)\n",
    "# Asegurarse que las fechas estén correctas\n",
    "final_table[\"year\"] = final_table[\"year\"] + 1960  # Añadir el año base para cada observación\n",
    "final_table[\"month\"] = final_table[\"month\"] + 1  # Asignar correctamente el mes\n",
    "print(final_table.head(40))\n",
    "# Revisa nuevamente el resultado de la consulta\n",
    "print(\"Verificación de NaNs en las anomalías:\")\n",
    "print(final_table.isnull().sum())  # Esto te ayudará a ver si los NaNs persisten\n",
    "\n",
    "\n",
    "# Guardar la tabla como Parquet\n",
    "regions_parquet = OUT / \"regions_timeseries_anomalies_detrended.parquet\"\n",
    "final_table.to_parquet(regions_parquet, index=False)\n",
    "\n",
    "print(\"OK ->\", regions_parquet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f3789cc6-491e-4156-b42b-844c7ffe5767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1960, 1, 0.12187341273642381, 'Nino34', 'HadISST_sst'), (1960, 2, -0.13970939875466173, 'Nino34', 'HadISST_sst'), (1960, 3, 0.051060818432104436, 'Nino34', 'HadISST_sst'), (1960, 4, 0.14372293118404975, 'Nino34', 'HadISST_sst'), (1960, 5, 0.20901033672964373, 'Nino34', 'HadISST_sst')]\n"
     ]
    }
   ],
   "source": [
    "con = duckdb.connect()\n",
    "# Cargar la tabla Parquet en DuckDB\n",
    "parquet_file = OUT / \"regions_timeseries_anomalies_detrended.parquet\"\n",
    "\n",
    "# Leer la tabla Parquet y cargarla en DuckDB\n",
    "con.execute(f\"CREATE OR REPLACE TABLE regions AS SELECT * FROM read_parquet('{parquet_file}')\")\n",
    "\n",
    "# Hacer una consulta SQL para ver las primeras filas\n",
    "result = con.execute(\"SELECT * FROM regions LIMIT 5\").fetchall()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ff2325-19bf-4fbd-a832-bd4de16213ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
